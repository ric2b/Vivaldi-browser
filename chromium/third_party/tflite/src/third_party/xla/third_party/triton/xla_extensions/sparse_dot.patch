diff --git a/include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td b/include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td
index 04ba95196..192d8fab4 100644
--- a/include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td
+++ b/include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td
@@ -1349,6 +1349,18 @@ elements along the K dim, or they use all elements of the tensor along the K dim
   }];
 }
 
+def SparseDotMetaEncodingAttr : DistributedEncoding<"SparseDotMetaEncoding", "sparse_dot_meta_encoding"> {
+  let mnemonic = "sparse_dot_meta";
+
+  let parameters = (ins "Attribute":$parent);
+  let assemblyFormat = "`<``{` struct(params) `}``>`";
+  let extraClassDeclaration = extraDistributedDeclaration # [{
+    SmallVector<unsigned> getContigPerThread() {
+      return getSizePerThread();
+    };
+  }];
+}
+
 def TTG_SharedMemorySpace : AttrDef<TritonGPU_Dialect, "SharedMemorySpace"> {
   let mnemonic = "shared_memory";
   let description = [{
diff --git a/include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td b/include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td
index a87e1c44a..456a4f224 100644
--- a/include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td
+++ b/include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td
@@ -7,6 +7,7 @@ include "triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td"
 include "mlir/Dialect/Arith/IR/ArithBase.td"
 include "triton/Dialect/Triton/IR/TritonTypes.td"
 include "triton/Dialect/Triton/IR/TritonAttrDefs.td"
+include "triton/Dialect/Triton/IR/TritonTypeInterfaces.td"
 include "mlir/IR/OpBase.td"
 include "mlir/Interfaces/SideEffectInterfaces.td" // Pure
 include "mlir/Interfaces/InferTypeOpInterface.td" // SameOperandsAndResultType
@@ -256,4 +257,19 @@ def TTG_LocalStoreOp : TTG_Op<"local_store", [DeclareOpInterfaceMethods<MemoryEf
   }];
 }
 
+def TTNG_SparseDotOp : TTG_Op<"sparse_dot", [
+        Pure, DeclareOpInterfaceMethods<InferTypeOpInterface>,
+        TypesMatchWith<"result's type matches accumulator's type", "d", "c", "$_self">]> {
+    let summary = "sparse dot";
+
+    let arguments = (ins
+      TT_TensorOrMemDesc:$a,
+      TT_TensorOrMemDesc:$b,
+      TT_FpIntTensor:$c,
+      TT_IntTensor: $aMeta);
+    let results = (outs TT_FpIntTensor:$d);
+    let assemblyFormat = "$a`,` $b`,` $c`,` $aMeta attr-dict `:` type($a) `meta` type($aMeta) `*` type($b) `->` type($d)";
+    let hasVerifier = 1;
+}
+
 #endif
diff --git a/lib/Dialect/TritonGPU/IR/Dialect.cpp b/lib/Dialect/TritonGPU/IR/Dialect.cpp
index 012786dae..6043b764a 100644
--- a/lib/Dialect/TritonGPU/IR/Dialect.cpp
+++ b/lib/Dialect/TritonGPU/IR/Dialect.cpp
@@ -497,6 +497,123 @@ getDefaultBlockedEncoding(MLIRContext *context, ArrayRef<int64_t> shape,
   return encoding;
 }
 
+///--- SparseDotOp ---
+namespace {
+// Implied properties of 2:4 sparse dots.
+constexpr int kContractingFactor = 2;
+constexpr int kMetadataElementsPerPackedValue = 8;
+constexpr int kMetadataElementsPerWarp = 16;
+}  // namespace
+
+mlir::LogicalResult SparseDotOp::inferReturnTypes(
+    MLIRContext *context, std::optional<Location> location, ValueRange operands,
+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,
+    SmallVectorImpl<Type> &inferredReturnTypes) {
+  return DotOp::inferReturnTypes(context, location, operands, attributes,
+                                 properties, regions, inferredReturnTypes);
+}
+
+LogicalResult SparseDotOp::verify() {
+  // Verify operand A.
+  auto aTensorTy = cast<TensorOrMemDesc>(getOperand(0).getType());
+  auto aElemTy = aTensorTy.getElementType();
+  if (!aElemTy.isF16() && !aElemTy.isBF16())
+    return emitError("element type of operand A is not supported");
+  auto aShape = aTensorTy.getShape();
+  if (aShape.size() != 2) return emitError("shape of operand A is incorrect");
+
+  // Verify operand B.
+  auto bTensorTy = cast<TensorOrMemDesc>(getOperand(1).getType());
+  auto bElemTy = bTensorTy.getElementType();
+  if (!bElemTy.isF16() && !bElemTy.isBF16())
+    return emitError("element type of operand B is not supported");
+  auto bShape = bTensorTy.getShape();
+  if (bShape.size() != 2) return emitError("shape of operand B is incorrect");
+
+  // Verify operand C.
+  auto cTensorTy = cast<RankedTensorType>(getOperand(2).getType());
+  auto cElemTy = cTensorTy.getElementType();
+  if (!cElemTy.isF32())
+    return emitError("element type of operand C is not supported");
+  auto cShape = cTensorTy.getShape();
+  if (cShape.size() != 2) return emitError("shape of operand C is incorrect");
+
+  // Check operand dependencies.
+  if (aShape[0] != cShape[0] || bShape[1] != cShape[1] ||
+      bShape[0] != aShape[1] * kContractingFactor)
+    return emitError("operand shape dimensions are incorrect");
+  if (aElemTy != bElemTy)
+    return emitError("operand element types do not match");
+
+  // Verify sparse metadata.
+  auto metaTy = cast<RankedTensorType>(getOperand(3).getType());
+  auto metaShape = metaTy.getShape();
+  if (!metaTy.getElementType().isInteger(16) || metaShape.size() != 2)
+    return emitError("sparse metadata tensor is invalid");
+  if (metaShape[0] != aShape[0] ||
+      metaShape[1] * kMetadataElementsPerPackedValue != aShape[1])
+    return emitError("sparse metadata shape dimensions are incorrect");
+
+  // Verify tensor encoding.
+  auto aEncoding = aTensorTy.getEncoding();
+  auto bEncoding = bTensorTy.getEncoding();
+  if (!aEncoding && !bEncoding) return mlir::success();
+  if (!aEncoding || !bEncoding)
+    return emitError("mismatching encoding between A and B operands");
+
+  Dialect &dialect = aEncoding.getDialect();
+  auto interface = cast<DialectInferLayoutInterface>(&dialect);
+  return interface->verifyDotOpEncodingCompatibility(getOperation(), aEncoding,
+                                                     bEncoding);
+}
+
+//--- SparseDotMetaEncodingAttr ---
+unsigned SparseDotMetaEncodingAttr::getTotalElemsPerThread(
+    ArrayRef<int64_t> shape, Type eltTy) const {
+  auto mmaLayout = mlir::cast<NvidiaMmaEncodingAttr>(getParent());
+  return product<int64_t>(shape) /
+         (mmaLayout.getWarpsPerCTA()[0] * kMetadataElementsPerWarp);
+}
+
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getElemsPerThread(
+    ArrayRef<int64_t> shape, Type eltTy) const {
+  llvm_unreachable("getElemsPerThread is not supported for sparse dot meta");
+  return SmallVector<unsigned>();
+}
+
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getCTAsPerCGA() const {
+  return ::getCTAsPerCGA(getParent());
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getCTAOrder() const {
+  return ::getCTAOrder(getParent());
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getCTASplitNum() const {
+  return ::getCTASplitNum(getParent());
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getWarpsPerCTA() const {
+  return ::getWarpsPerCTA(getParent());
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getWarpOrder() const {
+  return {1, 0};
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getThreadsPerWarp() const {
+  return ::getThreadsPerWarp(getParent());
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getThreadOrder() const {
+  return {1, 0};
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getSizePerThread() const {
+  return ::getSizePerThread(getParent());
+}
+SmallVector<unsigned> SparseDotMetaEncodingAttr::getShapePerCTATile(
+    ArrayRef<int64_t> tensorShape) const {
+  return ::getShapePerCTATile(getParent(), tensorShape);
+}
+std::optional<LinearLayout> SparseDotMetaEncodingAttr::toLinearLayout(
+    ArrayRef<int64_t> shape) const {
+  return ::toLinearLayout(shape, getParent());
+}
+
 } // namespace gpu
 } // namespace triton
 } // namespace mlir
diff --git a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
index eb26ffe3b..ba87d671e 100644
--- a/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp
@@ -39,8 +39,8 @@ static int getMMAVersionSafe(int computeCapability, DotOp op) {
   return 0;
 }
 
-SmallVector<unsigned> warpsPerTileV2(DotOp dotOp, const ArrayRef<int64_t> shape,
-                                     int numWarps) {
+SmallVector<unsigned>
+warpsPerTileV2(Operation *dotOp, const ArrayRef<int64_t> shape, int numWarps) {
   auto rank = shape.size();
   // Early exit for batched matmul
   if (rank == 3)
@@ -53,9 +53,8 @@ SmallVector<unsigned> warpsPerTileV2(DotOp dotOp, const ArrayRef<int64_t> shape,
   auto slices = multiRootGetSlice(dotOp, {filter}, {filter});
   bool hasChainedDot = false;
   for (Operation *op : slices) {
-    if (isa<DotOp>(op) && (op != dotOp)) {
-      auto chainedDot = cast<DotOp>(op);
-      auto resTy = chainedDot.getResult().getType();
+    if (dotOp->getName() == op->getName() && op != dotOp) {
+      auto resTy = cast<RankedTensorType>(op->getResult(0).getType());
       if (resTy.getRank() != rank) {
         continue;
       }
@@ -99,12 +98,13 @@ SmallVector<unsigned> warpsPerTileV2(DotOp dotOp, const ArrayRef<int64_t> shape,
 }
 
 SmallVector<unsigned, 2>
-warpsPerTileV3(DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,
+warpsPerTileV3(Operation *dotOp, const ArrayRef<int64_t> shape, int numWarps,
                const SmallVector<unsigned, 3> &instrShape) {
   SetVector<Operation *> slices;
-  mlir::getForwardSlice(dotOp.getResult(), &slices);
-  if (llvm::find_if(slices, [](Operation *op) { return isa<DotOp>(op); }) !=
-      slices.end())
+  mlir::getForwardSlice(dotOp->getResult(0), &slices);
+  if (llvm::find_if(slices, [&](Operation *op) {
+        return dotOp->getName() == op->getName();
+      }) != slices.end())
     return {(unsigned)numWarps, 1};
 
   // For MMAv3, the smallest indivisible unit of warp shape is (4, 1).
@@ -193,6 +193,7 @@ class BlockedToMMA : public mlir::OpRewritePattern<DotOp> {
                 mlir::TypeID::get<arith::ArithDialect>());
   }
 
+public:
   // Finds the first different bitwidth in the chain of shape-preserving
   // unary ops that x depends on.
   // There are two primary scenarios:
@@ -232,7 +233,7 @@ public:
   }
 
   static SmallVector<unsigned, 3>
-  getWarpsPerTile(DotOp dotOp, const ArrayRef<int64_t> shape, int version,
+  getWarpsPerTile(Operation *dotOp, const ArrayRef<int64_t> shape, int version,
                   int numWarps, const SmallVector<unsigned, 3> &instrShape) {
     switch (version) {
     case 2:
@@ -431,6 +432,21 @@ public:
   }
 };
 
+// Expose helper functions from BlockedToMMA to be reused for sparse matmul.
+SmallVector<unsigned, 3>
+getWarpsPerTile(Operation *dotOp, ArrayRef<int64_t> shape, int version,
+                int numWarps, const SmallVector<unsigned, 3> &instrShape) {
+  return BlockedToMMA::getWarpsPerTile(dotOp, shape, version, numWarps,
+                                       instrShape);
+}
+int computeOrigBitWidth(Value x) {
+  return BlockedToMMA::computeOrigBitWidth(x);
+}
+Value getSharedMemMMAOperand(Value v, mlir::PatternRewriter &rewriter,
+                                int opIdx, bool allowTranspose) {
+  return getSharedMemoryMMAOperand(v, rewriter, opIdx, allowTranspose);
+}
+
 } // namespace gpu
 } // namespace triton
 } // namespace mlir
diff --git a/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp b/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp
index d74e0a224..4e45f7c4c 100644
--- a/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp
+++ b/lib/Dialect/TritonGPU/Transforms/Pipeliner/MatmulLoopPipeline.cpp
@@ -51,6 +51,10 @@ struct LoadInfo {
 
 } // namespace
 
+static bool isDotLikeOp(Operation* op) {
+  return op->hasTrait<OpTrait::DotLike>() || isa<ttg::SparseDotOp>(op);
+}
+
 static void createAsyncCopy(scf::ForOp &forOp, tt::LoadOp loadOp, Value alloc,
                             Value insertIdx, Value extractIdx,
                             tt::CoarseSchedule &schedule,
@@ -235,17 +239,25 @@ getSharedEncIfAllUsersAreDotEnc(Value val, bool &incompatible) {
     } else {
       if (!isa<ttg::LocalLoadOp, ttg::ConvertLayoutOp>(user))
         return std::nullopt;
-      auto dotOpEnc = dyn_cast<ttg::DotOperandEncodingAttr>(
-          cast<TensorOrMemDesc>(user->getResult(0).getType()).getEncoding());
-      if (!dotOpEnc)
+      auto enc =
+          cast<TensorOrMemDesc>(user->getResult(0).getType()).getEncoding();
+      if (isa<ttg::DotOperandEncodingAttr>(enc)) {
+        auto srcTy = cast<TensorOrMemDesc>(val.getType());
+        auto CTALayout = ttg::getCTALayout(srcTy.getEncoding());
+        auto order = ttg::getOrder(srcTy.getEncoding());
+        unsigned bitWidth = srcTy.getElementType().getIntOrFloatBitWidth();
+        tempAttr = ttg::SharedEncodingAttr::get(
+            val.getContext(), cast<ttg::DotOperandEncodingAttr>(enc),
+            srcTy.getShape(), order, CTALayout, bitWidth, /*needTrans=*/false);
+      } else if (isa<ttg::SparseDotMetaEncodingAttr>(enc)) {
+        auto srcTy = cast<TensorOrMemDesc>(val.getType());
+        tempAttr = ttg::SharedEncodingAttr::get(
+            val.getContext(), /*vec=*/1, /*perPhase=*/1, /*maxPhase=*/1,
+            ttg::getOrder(srcTy.getEncoding()),
+            ttg::getCTALayout(srcTy.getEncoding()));
+      } else {
         return std::nullopt;
-      auto srcTy = cast<TensorOrMemDesc>(val.getType());
-      auto CTALayout = ttg::getCTALayout(srcTy.getEncoding());
-      auto order = ttg::getOrder(srcTy.getEncoding());
-      unsigned bitWidth = srcTy.getElementType().getIntOrFloatBitWidth();
-      tempAttr = ttg::SharedEncodingAttr::get(
-          val.getContext(), dotOpEnc, srcTy.getShape(), order, CTALayout,
-          bitWidth, /*needTrans=*/false);
+      }
     }
     // Check that the shared encodings needed by the users are compatible.
     if (attr != nullptr)
@@ -352,7 +364,7 @@ loadOpsToIndirectionLevelAndUse(scf::ForOp forOp) {
       };
 
   for (Operation &op : forOp.getBody()->without_terminator()) {
-    if (!op.hasTrait<OpTrait::DotLike>())
+    if (!isDotLikeOp(&op))
       continue;
     seen.clear();
     dfs(&op, 0, &op);
@@ -429,7 +441,7 @@ assignMemoryLayouts(llvm::SmallVector<std::tuple<Operation *, int, Operation *>>
         continue;
     }
 
-    if (use->hasTrait<OpTrait::DotLike>()) {
+    if (isDotLikeOp(use)) {
       loadInfo.usedByDot = true;
       if (loadIsMMAv3(op)) {
         loadInfo.loadIsMMAV3 = true;
diff --git a/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp b/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp
index fb0e7f6fd..37795c20c 100644
--- a/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp
+++ b/lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp
@@ -44,7 +44,7 @@ public:
       return;
     ModuleOp mod = getOperation();
     mod.walk([&](Operation *op) {
-      if (!isa<ttng::WarpGroupDotOp>(op))
+      if (!isa<ttng::WarpGroupDotOp, ttg::SparseDotOp>(op))
         return WalkResult::advance();
       OpBuilder builder(op);
       auto a = op->getOperand(0);
@@ -79,7 +79,7 @@ private:
     static DenseSet<std::pair<Operation *, unsigned>> trace;
     auto op = operand.getDefiningOp();
     // avoid redundant insertion
-    if (op && op->hasTrait<OpTrait::DotLike>())
+    if (op && (op->hasTrait<OpTrait::DotLike>() || isa<ttg::SparseDotOp>(op)))
       return false;
     // reach convertlayout
     if (op && isa<ttg::LocalAllocOp>(op) &&
diff --git a/third_party/nvidia/include/Dialect/NVGPU/IR/NVGPUOps.td b/third_party/nvidia/include/Dialect/NVGPU/IR/NVGPUOps.td
index 7affd8840..52aa2c131 100644
--- a/third_party/nvidia/include/Dialect/NVGPU/IR/NVGPUOps.td
+++ b/third_party/nvidia/include/Dialect/NVGPU/IR/NVGPUOps.td
@@ -87,6 +87,15 @@ def NVGPU_WGMMAOp : NVGPU_Op<"wgmma", []> {
   let assemblyFormat = "$opA `,` $opB `,` $useC (`,` $opC^)? attr-dict `:` functional-type(operands, $res)";
 }
 
+def NVGPU_SparseWGMMAOp : NVGPU_Op<"wgmma_sp", []> {
+  let arguments = (ins WGMMA_OperandType:$opA, I32:$metaA, WGMMA_OperandType:$opB, LLVM_AnyStruct:$opC,
+                   I32Attr:$m, I32Attr:$n, I32Attr:$k,
+                   WGMMA_EltTypeAttr:$eltTypeC, WGMMA_EltTypeAttr:$eltTypeA, WGMMA_EltTypeAttr:$eltTypeB,
+                   WGMMA_LayoutAttr:$layoutA, WGMMA_LayoutAttr:$layoutB);
+  let results = (outs LLVM_AnyStruct:$res);
+  let assemblyFormat = "$opA `meta` $metaA `,` $opB `,` $opC attr-dict `:` functional-type(operands, $res)";
+}
+
 def NVGPU_FenceAsyncSharedOp : NVGPU_Op<"fence_async_shared", []> {
   let arguments = (ins BoolAttr:$bCluster);
   let assemblyFormat = "attr-dict";
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
index baed96a29..e9d7f5859 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/DotOpToLLVM/WGMMA.cpp
@@ -88,8 +88,8 @@ int64_t getSwizzlingFromLayout(const SharedEncodingAttr &layout,
   return swizzlingByteWidth;
 }
 
-static Value createDescriptor(ConversionPatternRewriter &rewriter, Location loc,
-                              int64_t swizzling, uint32_t stride) {
+Value createDescriptor(ConversionPatternRewriter &rewriter, Location loc,
+                       int64_t swizzling, uint32_t stride) {
   // Create descriptor based on the format described in the spec:
   // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor
   union WGMMADescriptor {
diff --git a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TritonGPUToLLVM.cpp b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TritonGPUToLLVM.cpp
index f2742218e..4cb1fae93 100644
--- a/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TritonGPUToLLVM.cpp
+++ b/third_party/nvidia/lib/TritonNVIDIAGPUToLLVM/TritonGPUToLLVM.cpp
@@ -69,6 +69,7 @@ public:
     addIllegalDialect<triton::nvidia_gpu::TritonNvidiaGPUDialect>();
     addIllegalDialect<mlir::gpu::GPUDialect>();
     addLegalOp<mlir::UnrealizedConversionCastOp>();
+    addLegalOp<triton::gpu::SparseDotOp>();  // Rewritten in a separate pass.
   }
 };
 
