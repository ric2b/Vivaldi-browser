// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb --split-input-file | FileCheck %s --check-prefixes=CHECK,CHECK-%{PTX}

HloModule LargeReduction, is_scheduled=true

Sum {
    x.1 = c128[] parameter(0)
    y.1 = c128[] parameter(1)
    ROOT add.1 = c128[] add(x.1, y.1)
}

fused_computation {
  param_0 = c128[10000]{0} parameter(0)
  param_1 = c128[] parameter(1)
  ROOT out1.1 = c128[] reduce(c128[10000]{0} param_0, c128[] param_1), dimensions={0}, to_apply=Sum
}

ENTRY reduce.1 {
    parameter = c128[10000] parameter(0)
    init_value = c128[] constant((0, 0))
    ROOT wrapped_out1 = c128[] fusion(c128[10000]{0} parameter, c128[] init_value), kind=kInput, calls=fused_computation
}

// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = alloca %[[VAL_1:.*]], align 8
// CHECK:         %[[VAL_2:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_3:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_4:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_5:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_6:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_7:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_8:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_9:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_10:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_11:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_12:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_13:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_14:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_15:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_16:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_17:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_18:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_19:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_20:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_21:.*]] = alloca %[[VAL_1]], align 8
// CHECK:         %[[VAL_22:.*]] = alloca %[[VAL_1]], align 8
// CHECK-PTX:         %[[VAL_23:.*]] = alloca i32, align 4   
// CHECK-PTX:         %[[VAL_24:.*]] = alloca i32, align 4
// CHECK-DAG:         %[[VAL_25:.*]] = alloca %[[VAL_1]], align 8
// CHECK-DAG:         %[[VAL_26:.*]] = alloca i32, align 4
// CHECK-DAG:         %[[VAL_27:.*]] = alloca i32, align 4
// CHECK-DAG:         %[[VAL_28:.*]] = alloca %[[VAL_1]], align 8
// CHECK-DAG:         %[[VAL_29:.*]] = alloca %[[VAL_1]], align 8
// CHECK-PTX:     %[[VAL_30:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(), !range !2
// CHECK-GCN:     %[[VAL_30:.*]] = call i32 @llvm.amdgcn.workgroup.id.y
// CHECK:         %[[VAL_31:.*]] = icmp eq i32 %[[VAL_30]], 0
// CHECK:         br i1 %[[VAL_31]], label %[[VAL_32:.*]], label %[[VAL_33:.*]]
// CHECK:       reduce-group-0-after:                             ; preds = %thread_in_bounds-after, %[[VAL_34:.*]]
// CHECK:         ret void
// CHECK:       reduce-group-0-true:                              ; preds = %[[VAL_34]]
// CHECK:         %[[VAL_35:.*]] = load %[[VAL_1]], ptr %[[VAL_36:.*]], align 1, !invariant.load !{{[0-9]}}
// CHECK:         store %[[VAL_1]] %[[VAL_35]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK-PTX:     %thread.id.x = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !4
// CHECK-GCN:     %thread.id.x = call i32 @llvm.amdgcn.workitem.id.x
// CHECK-PTX:     %block.id.x = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !2
// CHECK-GCN:     %block.id.x = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK:         %thread.id.2 = urem i32 %thread.id.x, 640
// CHECK:         %lane_id = urem i32 %thread.id.x, 32
// CHECK:         %[[VAL_37:.*]] = udiv i32 %block.id.x, 1
// CHECK:         %[[VAL_38:.*]] = urem i32 %[[VAL_37]], 1
// CHECK:         %[[VAL_39:.*]] = udiv i32 %block.id.x, 1
// CHECK:         %[[VAL_40:.*]] = urem i32 %[[VAL_39]], 1
// CHECK-PTX:     %[[VAL_41:.*]] = udiv i32 %block.id.x, 1
// CHECK-PTX:     %[[VAL_42:.*]] = urem i32 %[[VAL_41]], 1
// CHECK:         %[[VAL_43:.*]] = udiv i32 %block.id.x, 1
// CHECK-PTX:     %[[VAL_44:.*]] = icmp eq i32 %[[VAL_40]], 0
// CHECK-GCN:     %[[VAL_44:.*]] = icmp eq i32 %[[VAL_38]], 0
// CHECK-PTX:     %tile_bound.2 = select i1 %[[VAL_44]], i32 5000, i32 5120
// CHECK-GCN:     %tile_bound.2 = select i1 %[[VAL_44]], i32 10000, i32 10240
// CHECK:         %tile_origin.0 = mul i32 %[[VAL_43]], 1
// CHECK-PTX:     %tile_origin.1 = mul i32 %[[VAL_42]], 1
// CHECK-GCN:     %tile_origin.1 = mul i32 %[[VAL_40]], 1
// CHECK-PTX:     %tile_origin.2 = mul i32 %[[VAL_40]], 5120
// CHECK-GCN:     %tile_origin.2 = mul i32 %[[VAL_38]], 10240
// CHECK-PTX:     %tile_origin.3 = mul i32 %[[VAL_38]], 2
// CHECK-PTX:     %[[VAL_45:.*]] = icmp eq i32 5120, %tile_bound.2
// CHECK-GCN:     %[[VAL_45:.*]] = icmp eq i32 10240, %tile_bound.2
// CHECK:         br i1 %[[VAL_45]], label %[[VAL_46:.*]], label %[[VAL_47:.*]]
// CHECK:       is_full_tile-after:                               ; preds = %[[VAL_48:.*]], %[[VAL_49:.*]]
// CHECK:         %[[VAL_50:.*]] = load i128, ptr{{.*}} %[[VAL_28]], align {{(16|8)}}
// CHECK:         %[[VAL_51:.*]] = bitcast i128 %[[VAL_50]] to <4 x i32>
// CHECK:         %[[VAL_52:.*]] = extractelement <4 x i32> %[[VAL_51]], i64 0
// CHECK-PTX:     %[[VAL_53:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_52]], i32 16, i32 31)
// CHECK-GCN:     %[[VAL_53:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_52]], i32 543)
// CHECK:         %[[VAL_54:.*]] = insertelement <4 x i32> %[[VAL_51]], i32 %[[VAL_53]], i64 0
// CHECK:         %[[VAL_55:.*]] = extractelement <4 x i32> %[[VAL_54]], i64 1
// CHECK-PTX:     %[[VAL_56:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_55]], i32 16, i32 31)
// CHECK-GCN:     %[[VAL_56:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_55]], i32 543)
// CHECK:         %[[VAL_57:.*]] = insertelement <4 x i32> %[[VAL_54]], i32 %[[VAL_56]], i64 1
// CHECK:         %[[VAL_58:.*]] = extractelement <4 x i32> %[[VAL_57]], i64 2
// CHECK-PTX:     %[[VAL_59:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_58]], i32 16, i32 31)
// CHECK-GCN:     %[[VAL_59:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_58]], i32 543)
// CHECK:         %[[VAL_60:.*]] = insertelement <4 x i32> %[[VAL_57]], i32 %[[VAL_59]], i64 2
// CHECK:         %[[VAL_61:.*]] = extractelement <4 x i32> %[[VAL_60]], i64 3
// CHECK-PTX:     %[[VAL_62:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_61]], i32 16, i32 31)
// CHECK-GCN:     %[[VAL_62:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_61]], i32 543)
// CHECK:         %[[VAL_63:.*]] = insertelement <4 x i32> %[[VAL_60]], i32 %[[VAL_62]], i64 3
// CHECK:         %[[VAL_64:.*]] = bitcast <4 x i32> %[[VAL_63]] to i128
// CHECK:         store i128 %[[VAL_64]], ptr{{.*}} %[[VAL_21]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_65_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_65_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_21]] to ptr
// CHECK-GCN:     %[[VAL_65_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_20]] to ptr
// CHECK-GCN:     call void @[[SUM:Sum.*]](ptr %[[VAL_65_1]], ptr %[[VAL_65_2]], ptr %[[VAL_65_3]])
// CHECK-PTX:     call void @[[SUM:Sum.*]](ptr %[[VAL_28]], ptr %[[VAL_21]], ptr %[[VAL_20]])
// CHECK:         %[[VAL_65:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_20]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_65]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK:         %[[VAL_66:.*]] = load i128, ptr{{.*}} %[[VAL_28]], align {{(16|8)}}
// CHECK:         %[[VAL_67:.*]] = bitcast i128 %[[VAL_66]] to <4 x i32>
// CHECK:         %[[VAL_68:.*]] = extractelement <4 x i32> %[[VAL_67]], i64 0
// CHECK-PTX:     %[[VAL_69:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_68]], i32 8, i32 31)
// CHECK-GCN:     %[[VAL_69:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_68]], i32 287)   
// CHECK:         %[[VAL_70:.*]] = insertelement <4 x i32> %[[VAL_67]], i32 %[[VAL_69]], i64 0
// CHECK:         %[[VAL_71:.*]] = extractelement <4 x i32> %[[VAL_70]], i64 1
// CHECK-PTX:     %[[VAL_72:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_71]], i32 8, i32 31)
// CHECK-GCN:     %[[VAL_72:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_71]], i32 287) 
// CHECK:         %[[VAL_73:.*]] = insertelement <4 x i32> %[[VAL_70]], i32 %[[VAL_72]], i64 1
// CHECK:         %[[VAL_74:.*]] = extractelement <4 x i32> %[[VAL_73]], i64 2
// CHECK-PTX:     %[[VAL_75:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_74]], i32 8, i32 31)
// CHECK-GCN:     %[[VAL_75:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_74]], i32 287) 
// CHECK:         %[[VAL_76:.*]] = insertelement <4 x i32> %[[VAL_73]], i32 %[[VAL_75]], i64 2
// CHECK:         %[[VAL_77:.*]] = extractelement <4 x i32> %[[VAL_76]], i64 3
// CHECK-PTX:     %[[VAL_78:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_77]], i32 8, i32 31)
// CHECK-GCN:     %[[VAL_78:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_77]], i32 287) 
// CHECK:         %[[VAL_79:.*]] = insertelement <4 x i32> %[[VAL_76]], i32 %[[VAL_78]], i64 3
// CHECK:         %[[VAL_80:.*]] = bitcast <4 x i32> %[[VAL_79]] to i128
// CHECK:         store i128 %[[VAL_80]], ptr{{.*}} %[[VAL_19]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_81_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_81_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_19]] to ptr
// CHECK-GCN:     %[[VAL_81_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_18]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_81_1]], ptr %[[VAL_81_2]], ptr %[[VAL_81_3]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_28]], ptr %[[VAL_19]], ptr %[[VAL_18]])
// CHECK:         %[[VAL_81:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_18]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_81]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK:         %[[VAL_82:.*]] = load i128, ptr{{.*}} %[[VAL_28]], align {{(16|8)}}
// CHECK:         %[[VAL_83:.*]] = bitcast i128 %[[VAL_82]] to <4 x i32>
// CHECK:         %[[VAL_84:.*]] = extractelement <4 x i32> %[[VAL_83]], i64 0
// CHECK-PTX:     %[[VAL_85:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_84]], i32 4, i32 31)
// CHECK-GCN:     %[[VAL_85:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_84]], i32 159)  
// CHECK:         %[[VAL_86:.*]] = insertelement <4 x i32> %[[VAL_83]], i32 %[[VAL_85]], i64 0
// CHECK:         %[[VAL_87:.*]] = extractelement <4 x i32> %[[VAL_86]], i64 1
// CHECK-PTX:     %[[VAL_88:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_87]], i32 4, i32 31)
// CHECK-GCN:     %[[VAL_88:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_87]], i32 159)  
// CHECK:         %[[VAL_89:.*]] = insertelement <4 x i32> %[[VAL_86]], i32 %[[VAL_88]], i64 1
// CHECK:         %[[VAL_90:.*]] = extractelement <4 x i32> %[[VAL_89]], i64 2
// CHECK-PTX:     %[[VAL_91:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_90]], i32 4, i32 31)
// CHECK-GCN:     %[[VAL_91:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_90]], i32 159)  
// CHECK:         %[[VAL_92:.*]] = insertelement <4 x i32> %[[VAL_89]], i32 %[[VAL_91]], i64 2
// CHECK:         %[[VAL_93:.*]] = extractelement <4 x i32> %[[VAL_92]], i64 3
// CHECK-PTX:     %[[VAL_94:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_93]], i32 4, i32 31)
// CHECK-GCN:     %[[VAL_94:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_93]], i32 159)  
// CHECK:         %[[VAL_95:.*]] = insertelement <4 x i32> %[[VAL_92]], i32 %[[VAL_94]], i64 3
// CHECK:         %[[VAL_96:.*]] = bitcast <4 x i32> %[[VAL_95]] to i128
// CHECK:         store i128 %[[VAL_96]], ptr{{.*}} %[[VAL_17]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_98_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_98_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_17]] to ptr
// CHECK-GCN:     %[[VAL_98_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_16]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_98_1]], ptr %[[VAL_98_2]], ptr %[[VAL_98_3]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_28]], ptr %[[VAL_17]], ptr %[[VAL_16]])
// CHECK:         %[[VAL_97:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_16]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_97]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK:         %[[VAL_98:.*]] = load i128, ptr{{.*}} %[[VAL_28]], align {{(16|8)}}
// CHECK:         %[[VAL_99:.*]] = bitcast i128 %[[VAL_98]] to <4 x i32>
// CHECK:         %[[VAL_100:.*]] = extractelement <4 x i32> %[[VAL_99]], i64 0
// CHECK-PTX:     %[[VAL_101:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_100]], i32 2, i32 31)
// CHECK-GCN:     %[[VAL_101:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_100]], i32 95)
// CHECK:         %[[VAL_102:.*]] = insertelement <4 x i32> %[[VAL_99]], i32 %[[VAL_101]], i64 0
// CHECK:         %[[VAL_103:.*]] = extractelement <4 x i32> %[[VAL_102]], i64 1
// CHECK-PTX:     %[[VAL_104:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_103]], i32 2, i32 31)
// CHECK-GCN:     %[[VAL_104:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_103]], i32 95)
// CHECK:         %[[VAL_105:.*]] = insertelement <4 x i32> %[[VAL_102]], i32 %[[VAL_104]], i64 1
// CHECK:         %[[VAL_106:.*]] = extractelement <4 x i32> %[[VAL_105]], i64 2
// CHECK-PTX:     %[[VAL_107:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_106]], i32 2, i32 31)
// CHECK-GCN:     %[[VAL_107:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_106]], i32 95)
// CHECK:         %[[VAL_108:.*]] = insertelement <4 x i32> %[[VAL_105]], i32 %[[VAL_107]], i64 2
// CHECK:         %[[VAL_109:.*]] = extractelement <4 x i32> %[[VAL_108]], i64 3
// CHECK-PTX:     %[[VAL_110:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_109]], i32 2, i32 31)
// CHECK-GCN:     %[[VAL_110:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_109]], i32 95)
// CHECK:         %[[VAL_111:.*]] = insertelement <4 x i32> %[[VAL_108]], i32 %[[VAL_110]], i64 3
// CHECK:         %[[VAL_112:.*]] = bitcast <4 x i32> %[[VAL_111]] to i128
// CHECK:         store i128 %[[VAL_112]], ptr{{.*}} %[[VAL_15]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_113_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_113_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_15]] to ptr
// CHECK-GCN:     %[[VAL_113_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_14]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_113_1]], ptr %[[VAL_113_2]], ptr %[[VAL_113_3]])
// CHECK_PTX:     call void @[[SUM]](ptr %[[VAL_28]], ptr %[[VAL_15]], ptr %[[VAL_14]])
// CHECK:         %[[VAL_113:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_14]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_113]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK:         %[[VAL_114:.*]] = load i128, ptr{{.*}} %[[VAL_28]], align {{(16|8)}}
// CHECK:         %[[VAL_115:.*]] = bitcast i128 %[[VAL_114]] to <4 x i32>
// CHECK:         %[[VAL_116:.*]] = extractelement <4 x i32> %[[VAL_115]], i64 0
// CHECK-PTX:     %[[VAL_117:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_116]], i32 1, i32 31)
// CHECK-GCN:     %[[VAL_117:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_116]], i32 63)
// CHECK:         %[[VAL_118:.*]] = insertelement <4 x i32> %[[VAL_115]], i32 %[[VAL_117]], i64 0
// CHECK:         %[[VAL_119:.*]] = extractelement <4 x i32> %[[VAL_118]], i64 1
// CHECK-PTX:     %[[VAL_120:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_119]], i32 1, i32 31)
// CHECK-GCN:     %[[VAL_120:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_119]], i32 63)
// CHECK:         %[[VAL_121:.*]] = insertelement <4 x i32> %[[VAL_118]], i32 %[[VAL_120]], i64 1
// CHECK:         %[[VAL_122:.*]] = extractelement <4 x i32> %[[VAL_121]], i64 2
// CHECK-PTX:     %[[VAL_123:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_122]], i32 1, i32 31)
// CHECK-GCN:     %[[VAL_123:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_122]], i32 63)
// CHECK:         %[[VAL_124:.*]] = insertelement <4 x i32> %[[VAL_121]], i32 %[[VAL_123]], i64 2
// CHECK:         %[[VAL_125:.*]] = extractelement <4 x i32> %[[VAL_124]], i64 3
// CHECK-PTX:     %[[VAL_126:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_125]], i32 1, i32 31)
// CHECK-GCN:     %[[VAL_126:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_125]], i32 63)
// CHECK:         %[[VAL_127:.*]] = insertelement <4 x i32> %[[VAL_124]], i32 %[[VAL_126]], i64 3
// CHECK:         %[[VAL_128:.*]] = bitcast <4 x i32> %[[VAL_127]] to i128
// CHECK:         store i128 %[[VAL_128]], ptr{{.*}} %[[VAL_13]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_129_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_129_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_13]] to ptr
// CHECK-GCN:     %[[VAL_129_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_12]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_129_1]], ptr %[[VAL_129_2]], ptr %[[VAL_129_3]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_28]], ptr %[[VAL_13]], ptr %[[VAL_12]])
// CHECK:         %[[VAL_129:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_12]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_129]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK:         %[[VAL_130:.*]] = udiv i32 %thread.id.2, 32
// CHECK:         br i1 true, label %thread_in_bounds-true, label %thread_in_bounds-after

// CHECK:       thread_in_bounds-after:                           ; preds = %[[VAL_131:.*]], %[[VAL_132:.*]]
// CHECK:         br label %[[VAL_33]]

// CHECK:       is_full_tile-true:                                ; preds = %[[VAL_32]]
// CHECK:         store i32 0, ptr{{.*}} %[[VAL_27]], align 4
// CHECK:         br label %[[VAL_133:.*]]

// CHECK:       loop2.loop_header:                                ; preds = %[[VAL_134:.*]], %[[VAL_46]]
// CHECK:         %[[VAL_135:.*]] = load i32, ptr{{.*}} %[[VAL_27]], align 4
// CHECK-PTX:     %[[VAL_136:.*]] = icmp uge i32 %[[VAL_135]], 5120
// CHECK-GCN:     %[[VAL_136:.*]] = icmp uge i32 %[[VAL_135]], 10240
// CHECK:         br i1 %[[VAL_136]], label %[[VAL_49]], label %[[VAL_137:.*]]

// CHECK:       loop2.loop_body:                                  ; preds = %[[VAL_133]]
// CHECK:         %[[VAL_138:.*]] = add nuw nsw i32 %[[VAL_135]], 640
// CHECK:         store i32 %[[VAL_138]], ptr{{.*}} %[[VAL_27]], align 4
// CHECK:         %[[VAL_140:.*]] = add i32 %[[VAL_135]], %thread.id.2
// CHECK-GCN:     %[[VAL_147:.*]] = add i32 %tile_origin.0, 0
// CHECK-GCN:     %[[VAL_148:.*]] = add i32 %tile_origin.1, 0
// CHECK-GCN:     %[[VAL_149:.*]] = add i32 %tile_origin.2, %[[VAL_140]]
// CHECK-GCN:     %[[VAL_160:.*]] = getelementptr inbounds [10000 x %[[VAL_1]]], ptr %[[VAL_161:.*]], i32 0, i32 %[[VAL_149]]
// CHECK-GCN:     %[[VAL_162:.*]] = load %[[VAL_1]], ptr %[[VAL_160]], align 1, !invariant.load !2
// CHECK-GCN:     store %[[VAL_1]] %[[VAL_162]], ptr{{.*}} %[[VAL_29]], align 1
// CHECK-GCN:     %[[VAL_163_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_163_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_29]] to ptr
// CHECK-GCN:     %[[VAL_163_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_25]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_163_1]], ptr %[[VAL_163_2]], ptr %[[VAL_163_3]])
// CHECK-GCN:     %[[VAL_163:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_25]], align 1
// CHECK-GCN:     store %[[VAL_1]] %[[VAL_163]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK-PTX:     store i32 0, ptr %[[VAL_26]], align 4
// CHECK:         br label %[[VAL_141:.*]]

// CHECK-PTX:   loop3.loop_header:                                ; preds = %[[VAL_142:.*]], %[[VAL_137]]
// CHECK-PTX:     %[[VAL_143:.*]] = load i32, ptr %[[VAL_26]], align 4
// CHECK-PTX:     %[[VAL_144:.*]] = icmp uge i32 %[[VAL_143]], 2
// CHECK-PTX:     br i1 %[[VAL_144]], label %[[VAL_134]], label %[[VAL_142]]

// CHECK-PTX:   loop3.loop_body:                                  ; preds = %[[VAL_141]]
// CHECK-PTX:     %[[VAL_145:.*]] = add nuw nsw i32 %[[VAL_143]], 1
// CHECK-PTX:     store i32 %[[VAL_145]], ptr %[[VAL_26]], align 4
// CHECK-PTX:     %[[VAL_147:.*]] = add i32 %tile_origin.0, 0
// CHECK-PTX:     %[[VAL_148:.*]] = add i32 %tile_origin.1, 0
// CHECK-PTX:     %[[VAL_149:.*]] = add i32 %tile_origin.2, %[[VAL_140]]
// CHECK-PTX:     %[[VAL_150:.*]] = add i32 %tile_origin.3, %[[VAL_143]]
// CHECK-PTX:     %[[VAL_151:.*]] = mul nuw nsw i32 %[[VAL_150]], 1
// CHECK-PTX:     %[[VAL_152:.*]] = add nuw nsw i32 0, %[[VAL_151]]
// CHECK-PTX:     %[[VAL_153:.*]] = mul nuw nsw i32 %[[VAL_149]], 2
// CHECK-PTX:     %[[VAL_154:.*]] = add nuw nsw i32 %[[VAL_152]], %[[VAL_153]]
// CHECK-PTX:     %[[VAL_155:.*]] = udiv i32 %[[VAL_154]], 10000
// CHECK-PTX:     %[[VAL_156:.*]] = mul nuw nsw i32 %[[VAL_148]], 1
// CHECK-PTX:     %[[VAL_157:.*]] = add nuw nsw i32 0, %[[VAL_156]]
// CHECK-PTX:     %[[VAL_158:.*]] = mul nuw nsw i32 %[[VAL_147]], 1
// CHECK-PTX:     %[[VAL_159:.*]] = add nuw nsw i32 0, %[[VAL_158]]
// CHECK-PTX:     %[[VAL_160:.*]] = getelementptr inbounds [10000 x %[[VAL_1]]], ptr %[[VAL_161:.*]], i32 0, i32 %[[VAL_154]]
// CHECK-PTX:     %[[VAL_162:.*]] = load %[[VAL_1]], ptr %[[VAL_160]], align 1, !invariant.load !3
// CHECK-PTX:     store %[[VAL_1]] %[[VAL_162]], ptr %[[VAL_29]], align 1
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_28]], ptr %[[VAL_29]], ptr %[[VAL_25]])
// CHECK-PTX:     %[[VAL_163:.*]] = load %[[VAL_1]], ptr %[[VAL_25]], align 1
// CHECK-PTX:     store %[[VAL_1]] %[[VAL_163]], ptr %[[VAL_28]], align 1
// CHECK-PTX:     br label %[[VAL_141]], !llvm.loop !5

// CHECK-PTX:   loop3.loop_exit:                                  ; preds = %[[VAL_141]]
// CHECK-PTX:     br label %[[VAL_133]], !llvm.loop !7

// CHECK:       loop2.loop_exit:                                  ; preds = %[[VAL_133]]
// CHECK:         br label %[[VAL_132]]

// CHECK:       is_full_tile-false:                               ; preds = %[[VAL_32]]
// CHECK-PTX:     store i32 0, ptr %[[VAL_24]], align 4
// CHECK-GCN:     store i32 0, ptr{{.*}} %[[VAL_26]], align 4
// CHECK:         br label %[[VAL_164:.*]]

// CHECK:       loop2.loop_header{{4|3}}:                               ; preds = %[[VAL_165:.*]], %[[VAL_47]]
// CHECK-PTX:     %[[VAL_166:.*]] = load i32, ptr %[[VAL_24]], align 4
// CHECK-PTX:     %[[VAL_167:.*]] = icmp uge i32 %[[VAL_166]], 5120
// CHECK-GCN:     %[[VAL_166:.*]] = load i32, ptr{{.*}} %[[VAL_26]], align 4
// CHECK-GCN:     %[[VAL_167:.*]] = icmp uge i32 %[[VAL_166]], 10240
// CHECK:         br i1 %[[VAL_167]], label %[[VAL_48]], label %[[VAL_168:.*]]

// CHECK:       loop2.loop_body{{5|4}}:                                 ; preds = %[[VAL_164]]
// CHECK:         %[[VAL_169:.*]] = add nuw nsw i32 %[[VAL_166]], 640
// CHECK-PTX:     store i32 %[[VAL_169]], ptr %[[VAL_24]], align 4
// CHECK-GCN:     store i32 %[[VAL_169]], ptr{{.*}} %[[VAL_26]], align 4
// CHECK:         %[[VAL_171:.*]] = add i32 %[[VAL_166]], %thread.id.2
// CHECK:         %[[VAL_172:.*]] = icmp ult i32 %[[VAL_171]], %tile_bound.2
// CHECK:         br i1 %[[VAL_172]], label %[[VAL_173:.*]], label %[[VAL_165]]

// CHECK:       x_in_tile-after:                                  ; preds = %[[VAL_174:.*]], %[[VAL_168]]
// CHECK:         br label %[[VAL_164]], !llvm.loop !{{9|7}}

// CHECK:       loop2.loop_exit{{3|2}}:                                 ; preds = %[[VAL_164]]
// CHECK:         br label %[[VAL_132]]

// CHECK:       x_in_tile-true:                                   ; preds = %[[VAL_168]]
// CHECK-GCN:     %[[VAL_181:.*]] = add i32 %tile_origin.0, 0
// CHECK-GCN:     %[[VAL_182:.*]] = add i32 %tile_origin.1, 0
// CHECK-GCN:     %[[VAL_183:.*]] = add i32 %tile_origin.2, %[[VAL_171]]
// CHECK-GCN:     %[[VAL_194:.*]] = getelementptr inbounds [10000 x %[[VAL_1]]], ptr %[[VAL_161]], i32 0, i32 %[[VAL_183]]
// CHECK-GCN:     %[[VAL_195:.*]] = load %[[VAL_1]], ptr %[[VAL_194]], align 1, !invariant.load !2
// CHECK-GCN:     store %[[VAL_1]] %[[VAL_195]], ptr{{.*}} %[[VAL_29]], align 1
// CHECK-GCN:     %[[VAL_196_1:.*]] = addrspacecast ptr{{.*}} %[[VAL_28]] to ptr
// CHECK-GCN:     %[[VAL_196_2:.*]] = addrspacecast ptr{{.*}} %[[VAL_29]] to ptr
// CHECK-GCN:     %[[VAL_196_3:.*]] = addrspacecast ptr{{.*}} %[[VAL_22]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_196_1]], ptr %[[VAL_196_2]], ptr %[[VAL_196_3]])
// CHECK-GCN:     %[[VAL_196:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_22]], align 1
// CHECK-GCN:     store %[[VAL_1]] %[[VAL_196]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK-PTX:     store i32 0, ptr %[[VAL_23]], align 4
// CHECK:         br label %[[VAL_175:.*]]   

// CHECK-PTX:   loop3.loop_header10:                              ; preds = %[[VAL_176:.*]], %[[VAL_173]]
// CHECK-PTX:     %[[VAL_177:.*]] = load i32, ptr %[[VAL_23]], align 4
// CHECK-PTX:     %[[VAL_178:.*]] = icmp uge i32 %[[VAL_177]], 2
// CHECK-PTX:     br i1 %[[VAL_178]], label %[[VAL_174]], label %[[VAL_176]]
// CHECK-PTX:   loop3.loop_body11:                                ; preds = %[[VAL_175]]
// CHECK-PTX:     %[[VAL_179:.*]] = add nuw nsw i32 %[[VAL_177]], 1
// CHECK-PTX:     store i32 %[[VAL_179]], ptr %[[VAL_23]], align 4
// CHECK-PTX:     %[[VAL_181:.*]] = add i32 %tile_origin.0, 0
// CHECK-PTX:     %[[VAL_182:.*]] = add i32 %tile_origin.1, 0
// CHECK-PTX:     %[[VAL_183:.*]] = add i32 %tile_origin.2, %[[VAL_171]]
// CHECK-PTX:     %[[VAL_184:.*]] = add i32 %tile_origin.3, %[[VAL_177]]
// CHECK-PTX:     %[[VAL_185:.*]] = mul nuw nsw i32 %[[VAL_184]], 1
// CHECK-PTX:     %[[VAL_186:.*]] = add nuw nsw i32 0, %[[VAL_185]]
// CHECK-PTX:     %[[VAL_187:.*]] = mul nuw nsw i32 %[[VAL_183]], 2
// CHECK-PTX:     %[[VAL_188:.*]] = add nuw nsw i32 %[[VAL_186]], %[[VAL_187]]
// CHECK-PTX:     %[[VAL_189:.*]] = udiv i32 %[[VAL_188]], 10000
// CHECK-PTX:     %[[VAL_190:.*]] = mul nuw nsw i32 %[[VAL_182]], 1
// CHECK-PTX:     %[[VAL_191:.*]] = add nuw nsw i32 0, %[[VAL_190]]
// CHECK-PTX:     %[[VAL_192:.*]] = mul nuw nsw i32 %[[VAL_181]], 1
// CHECK-PTX:     %[[VAL_193:.*]] = add nuw nsw i32 0, %[[VAL_192]]
// CHECK-PTX:     %[[VAL_194:.*]] = getelementptr inbounds [10000 x %[[VAL_1]]], ptr %[[VAL_161]], i32 0, i32 %[[VAL_188]]
// CHECK-PTX:     %[[VAL_195:.*]] = load %[[VAL_1]], ptr %[[VAL_194]], align 1, !invariant.load !3
// CHECK-PTX:     store %[[VAL_1]] %[[VAL_195]], ptr %[[VAL_29]], align 1
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_28]], ptr %[[VAL_29]], ptr %[[VAL_22]])
// CHECK-PTX:     %[[VAL_196:.*]] = load %[[VAL_1]], ptr %[[VAL_22]], align 1
// CHECK-PTX:     store %[[VAL_1]] %[[VAL_196]], ptr %[[VAL_28]], align 1
// CHECK-PTX:     br label %[[VAL_175]], !llvm.loop !10
// CHECK-PTX:   loop3.loop_exit9:                                 ; preds = %[[VAL_175]]
// CHECK-PTX:     br label %[[VAL_165]]

// CHECK:       thread_in_bounds-true:                            ; preds = %[[VAL_132]]
// CHECK:         %[[VAL_197:.*]] = icmp eq i32 %lane_id, 0
// CHECK:         br i1 %[[VAL_197]], label %[[VAL_198:.*]], label %[[VAL_199:.*]]
// CHECK:       intra_warp_reduce_write-after:                    ; preds = %[[VAL_198]], %thread_in_bounds-true
// CHECK-GCN:     fence syncscope("workgroup") seq_cst
// CHECK-GCN:     call void @llvm.amdgcn.s.barrier()
// CHECK-PTX:     call void @llvm.nvvm.barrier0()
// CHECK:         %[[VAL_200:.*]] = icmp eq i32 %[[VAL_130]], 0
// CHECK:         br i1 %[[VAL_200]], label %[[VAL_201:.*]], label %[[VAL_131]]
// CHECK:       inter_warp_reduce-after:                          ; preds = %[[VAL_202:.*]], %[[VAL_199]]
// CHECK:         br label %thread_in_bounds-after
// CHECK:       intra_warp_reduce_write-true:                     ; preds = %thread_in_bounds-true
// CHECK:         %[[VAL_203:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_28]], align 1
// CHECK:         %[[VAL_204:.*]] = getelementptr inbounds [1 x [20 x %[[VAL_1]]]], ptr addrspace(3) @shared_cache, i32 0, i32 0, i32 %[[VAL_130]]
// CHECK:         %[[VAL_205:.*]] = addrspacecast ptr addrspace(3) %[[VAL_204]] to ptr
// CHECK:         store %[[VAL_1]] %[[VAL_203]], ptr %[[VAL_205]], align 1
// CHECK:         br label %[[VAL_199]]
// CHECK:       inter_warp_reduce-true:                           ; preds = %[[VAL_199]]
// CHECK:         %[[VAL_206:.*]] = getelementptr inbounds [1 x [20 x %[[VAL_1]]]], ptr addrspace(3) @shared_cache, i32 0, i32 0, i32 %lane_id
// CHECK:         %[[VAL_207:.*]] = addrspacecast ptr addrspace(3) %[[VAL_206]] to ptr
// CHECK-GCN:     %[[VAL_207_1:.*]] = addrspacecast ptr addrspace(5) %[[VAL_11]] to ptr
// CHECK-GCN:     store %[[VAL_1]] %[[VAL_35]], ptr %[[VAL_207_1]], align 1
// CHECK-PTX:     store %[[VAL_1]] %[[VAL_35]], ptr %[[VAL_11]], align 1
// CHECK:         %[[VAL_208:.*]] = icmp ult i32 %thread.id.2, 20
// CHECK-GCN:     %[[VAL_209:.*]] = select i1 %[[VAL_208]], ptr %[[VAL_207]], ptr %[[VAL_207_1]]
// CHECK-PTX:     %[[VAL_209:.*]] = select i1 %[[VAL_208]], ptr %[[VAL_207]], ptr %[[VAL_11]]
// CHECK:         %[[VAL_210:.*]] = load i128, ptr %[[VAL_209]], align {{(16|8)}}
// CHECK:         %[[VAL_211:.*]] = bitcast i128 %[[VAL_210]] to <4 x i32>
// CHECK:         %[[VAL_212:.*]] = extractelement <4 x i32> %[[VAL_211]], i64 0
// CHECK-GCN:     %[[VAL_213:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_212]], i32 543)
// CHECK-PTX:     %[[VAL_213:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_212]], i32 16, i32 31)
// CHECK:         %[[VAL_214:.*]] = insertelement <4 x i32> %[[VAL_211]], i32 %[[VAL_213]], i64 0
// CHECK:         %[[VAL_215:.*]] = extractelement <4 x i32> %[[VAL_214]], i64 1
// CHECK-GCN:     %[[VAL_216:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_215]], i32 543)     
// CHECK-PTX:     %[[VAL_216:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_215]], i32 16, i32 31)
// CHECK:         %[[VAL_217:.*]] = insertelement <4 x i32> %[[VAL_214]], i32 %[[VAL_216]], i64 1
// CHECK:         %[[VAL_218:.*]] = extractelement <4 x i32> %[[VAL_217]], i64 2
// CHECK-GCN:     %[[VAL_219:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_218]], i32 543)
// CHECK-PTX:     %[[VAL_219:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_218]], i32 16, i32 31)
// CHECK:         %[[VAL_220:.*]] = insertelement <4 x i32> %[[VAL_217]], i32 %[[VAL_219]], i64 2
// CHECK:         %[[VAL_221:.*]] = extractelement <4 x i32> %[[VAL_220]], i64 3
// CHECK-GCN:     %[[VAL_222:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_221]], i32 543)
// CHECK-PTX:     %[[VAL_222:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_221]], i32 16, i32 31)
// CHECK:         %[[VAL_223:.*]] = insertelement <4 x i32> %[[VAL_220]], i32 %[[VAL_222]], i64 3
// CHECK:         %[[VAL_224:.*]] = bitcast <4 x i32> %[[VAL_223]] to i128
// CHECK:         store i128 %[[VAL_224]], ptr{{.*}} %[[VAL_10]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_225_1:.*]] = addrspacecast ptr addrspace(5) %[[VAL_10]] to ptr
// CHECK-GCN:     %[[VAL_225_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_9]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_225_1]], ptr %[[VAL_225_2]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_10]], ptr %[[VAL_9]])
// CHECK:         %[[VAL_225:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_9]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_225]], ptr %[[VAL_209]], align 1
// CHECK:         %[[VAL_226:.*]] = load i128, ptr %[[VAL_209]], align {{(16|8)}}
// CHECK:         %[[VAL_227:.*]] = bitcast i128 %[[VAL_226]] to <4 x i32>
// CHECK:         %[[VAL_228:.*]] = extractelement <4 x i32> %[[VAL_227]], i64 0
// CHECK-GCN:     %[[VAL_229:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_228]], i32 287)
// CHECK-PTX:     %[[VAL_229:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_228]], i32 8, i32 31)
// CHECK:         %[[VAL_230:.*]] = insertelement <4 x i32> %[[VAL_227]], i32 %[[VAL_229]], i64 0
// CHECK:         %[[VAL_231:.*]] = extractelement <4 x i32> %[[VAL_230]], i64 1
// CHECK-GCN:     %[[VAL_232:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_231]], i32 287)
// CHECK-PTX:     %[[VAL_232:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_231]], i32 8, i32 31)
// CHECK:         %[[VAL_233:.*]] = insertelement <4 x i32> %[[VAL_230]], i32 %[[VAL_232]], i64 1
// CHECK:         %[[VAL_234:.*]] = extractelement <4 x i32> %[[VAL_233]], i64 2
// CHECK-GCN:     %[[VAL_235:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_234]], i32 287)
// CHECK-PTX:     %[[VAL_235:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_234]], i32 8, i32 31)
// CHECK:         %[[VAL_236:.*]] = insertelement <4 x i32> %[[VAL_233]], i32 %[[VAL_235]], i64 2
// CHECK:         %[[VAL_237:.*]] = extractelement <4 x i32> %[[VAL_236]], i64 3
// CHECK-GCN:     %[[VAL_238:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_237]], i32 287)
// CHECK-PTX:     %[[VAL_238:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_237]], i32 8, i32 31)
// CHECK:         %[[VAL_239:.*]] = insertelement <4 x i32> %[[VAL_236]], i32 %[[VAL_238]], i64 3
// CHECK:         %[[VAL_240:.*]] = bitcast <4 x i32> %[[VAL_239]] to i128
// CHECK:         store i128 %[[VAL_240]], ptr{{.*}} %[[VAL_8]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_241_1:.*]] = addrspacecast ptr addrspace(5) %[[VAL_8]] to ptr
// CHECK-GCN:     %[[VAL_241_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_7]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_241_1]], ptr %[[VAL_241_2]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_8]], ptr %[[VAL_7]])
// CHECK:         %[[VAL_241:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_7]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_241]], ptr %[[VAL_209]], align 1
// CHECK:         %[[VAL_242:.*]] = load i128, ptr %[[VAL_209]], align {{(16|8)}}
// CHECK:         %[[VAL_243:.*]] = bitcast i128 %[[VAL_242]] to <4 x i32>
// CHECK:         %[[VAL_244:.*]] = extractelement <4 x i32> %[[VAL_243]], i64 0
// CHECK-GCN:     %[[VAL_245:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_244]], i32 159)
// CHECK-PTX:     %[[VAL_245:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_244]], i32 4, i32 31)
// CHECK:         %[[VAL_246:.*]] = insertelement <4 x i32> %[[VAL_243]], i32 %[[VAL_245]], i64 0
// CHECK:         %[[VAL_247:.*]] = extractelement <4 x i32> %[[VAL_246]], i64 1
// CHECK-GCN:     %[[VAL_248:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_247]], i32 159)
// CHECK-PTX:     %[[VAL_248:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_247]], i32 4, i32 31)
// CHECK:         %[[VAL_249:.*]] = insertelement <4 x i32> %[[VAL_246]], i32 %[[VAL_248]], i64 1
// CHECK:         %[[VAL_250:.*]] = extractelement <4 x i32> %[[VAL_249]], i64 2
// CHECK-GCN:     %[[VAL_251:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_250]], i32 159)
// CHECK-PTX:     %[[VAL_251:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_250]], i32 4, i32 31)
// CHECK:         %[[VAL_252:.*]] = insertelement <4 x i32> %[[VAL_249]], i32 %[[VAL_251]], i64 2
// CHECK:         %[[VAL_253:.*]] = extractelement <4 x i32> %[[VAL_252]], i64 3
// CHECK-GCN:     %[[VAL_254:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_253]], i32 159)
// CHECK-PTX:     %[[VAL_254:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_253]], i32 4, i32 31)
// CHECK:         %[[VAL_255:.*]] = insertelement <4 x i32> %[[VAL_252]], i32 %[[VAL_254]], i64 3
// CHECK:         %[[VAL_256:.*]] = bitcast <4 x i32> %[[VAL_255]] to i128
// CHECK:         store i128 %[[VAL_256]], ptr{{.*}} %[[VAL_6]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_257_1:.*]] = addrspacecast ptr addrspace(5) %[[VAL_6]] to ptr
// CHECK-GCN:     %[[VAL_257_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_5]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_257_1]], ptr %[[VAL_257_2]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_6]], ptr %[[VAL_5]])
// CHECK:         %[[VAL_257:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_5]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_257]], ptr{{.*}} %[[VAL_209]], align 1
// CHECK:         %[[VAL_258:.*]] = load i128, ptr %[[VAL_209]], align {{(16|8)}}
// CHECK:         %[[VAL_259:.*]] = bitcast i128 %[[VAL_258]] to <4 x i32>
// CHECK:         %[[VAL_260:.*]] = extractelement <4 x i32> %[[VAL_259]], i64 0
// CHECK-GCN:     %[[VAL_261:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_260]], i32 95)
// CHECK-PTX:     %[[VAL_261:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_260]], i32 2, i32 31)
// CHECK:         %[[VAL_262:.*]] = insertelement <4 x i32> %[[VAL_259]], i32 %[[VAL_261]], i64 0
// CHECK:         %[[VAL_263:.*]] = extractelement <4 x i32> %[[VAL_262]], i64 1
// CHECK-GCN:     %[[VAL_264:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_263]], i32 95)
// CHECK-PTX:     %[[VAL_264:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_263]], i32 2, i32 31)
// CHECK:         %[[VAL_265:.*]] = insertelement <4 x i32> %[[VAL_262]], i32 %[[VAL_264]], i64 1
// CHECK:         %[[VAL_266:.*]] = extractelement <4 x i32> %[[VAL_265]], i64 2
// CHECK-GCN:     %[[VAL_267:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_266]], i32 95)
// CHECK-PTX:     %[[VAL_267:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_266]], i32 2, i32 31)
// CHECK:         %[[VAL_268:.*]] = insertelement <4 x i32> %[[VAL_265]], i32 %[[VAL_267]], i64 2
// CHECK:         %[[VAL_269:.*]] = extractelement <4 x i32> %[[VAL_268]], i64 3
// CHECK-GCN:     %[[VAL_270:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_269]], i32 95)
// CHECK-PTX:     %[[VAL_270:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_269]], i32 2, i32 31)
// CHECK:         %[[VAL_271:.*]] = insertelement <4 x i32> %[[VAL_268]], i32 %[[VAL_270]], i64 3
// CHECK:         %[[VAL_272:.*]] = bitcast <4 x i32> %[[VAL_271]] to i128
// CHECK:         store i128 %[[VAL_272]], ptr{{.*}} %[[VAL_4]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_273_1:.*]] = addrspacecast ptr addrspace(5) %[[VAL_4]] to ptr
// CHECK-GCN:     %[[VAL_273_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_3]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_273_1]], ptr %[[VAL_273_2]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_4]], ptr %[[VAL_3]])
// CHECK:         %[[VAL_273:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_3]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_273]], ptr %[[VAL_209]], align 1
// CHECK:         %[[VAL_274:.*]] = load i128, ptr %[[VAL_209]], align {{(16|8)}}
// CHECK:         %[[VAL_275:.*]] = bitcast i128 %[[VAL_274]] to <4 x i32>
// CHECK:         %[[VAL_276:.*]] = extractelement <4 x i32> %[[VAL_275]], i64 0
// CHECK-GCN:     %[[VAL_277:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_276]], i32 63)
// CHECK-PTX:     %[[VAL_277:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_276]], i32 1, i32 31)
// CHECK:         %[[VAL_278:.*]] = insertelement <4 x i32> %[[VAL_275]], i32 %[[VAL_277]], i64 0
// CHECK:         %[[VAL_279:.*]] = extractelement <4 x i32> %[[VAL_278]], i64 1
// CHECK-GCN:     %[[VAL_280:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_279]], i32 63)
// CHECK-PTX:     %[[VAL_280:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_279]], i32 1, i32 31)
// CHECK:         %[[VAL_281:.*]] = insertelement <4 x i32> %[[VAL_278]], i32 %[[VAL_280]], i64 1
// CHECK:         %[[VAL_282:.*]] = extractelement <4 x i32> %[[VAL_281]], i64 2
// CHECK-GCN:     %[[VAL_283:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_282]], i32 63)
// CHECK-PTX:     %[[VAL_283:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_282]], i32 1, i32 31)
// CHECK:         %[[VAL_284:.*]] = insertelement <4 x i32> %[[VAL_281]], i32 %[[VAL_283]], i64 2
// CHECK:         %[[VAL_285:.*]] = extractelement <4 x i32> %[[VAL_284]], i64 3
// CHECK-GCN:     %[[VAL_286:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_285]], i32 63)
// CHECK-PTX:     %[[VAL_286:.*]] = call i32 @llvm.nvvm.shfl.sync.down.i32(i32 -1, i32 %[[VAL_285]], i32 1, i32 31)
// CHECK:         %[[VAL_287:.*]] = insertelement <4 x i32> %[[VAL_284]], i32 %[[VAL_286]], i64 3
// CHECK:         %[[VAL_288:.*]] = bitcast <4 x i32> %[[VAL_287]] to i128
// CHECK:         store i128 %[[VAL_288]], ptr{{.*}} %[[VAL_2]], align {{(16|8)}}
// CHECK-GCN:     %[[VAL_289_1:.*]] = addrspacecast ptr addrspace(5) %[[VAL_2]] to ptr
// CHECK-GCN:     %[[VAL_289_2:.*]] = addrspacecast ptr addrspace(5) %[[VAL_0]] to ptr
// CHECK-GCN:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_289_1]], ptr %[[VAL_289_2]])
// CHECK-PTX:     call void @[[SUM]](ptr %[[VAL_209]], ptr %[[VAL_2]], ptr %[[VAL_0]])
// CHECK:         %[[VAL_289:.*]] = load %[[VAL_1]], ptr{{.*}} %[[VAL_0]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_289]], ptr %[[VAL_209]], align 1
// CHECK:         %[[VAL_290:.*]] = icmp eq i32 %thread.id.2, 0
// CHECK:         br i1 %[[VAL_290]], label %[[VAL_291:.*]], label %[[VAL_202]]
// CHECK:       reduction_write_output-after:                     ; preds = %[[VAL_291]], %[[VAL_201]]
// CHECK:         br label %[[VAL_131]]
// CHECK:       reduction_write_output-true:                      ; preds = %[[VAL_201]]
// CHECK:         %[[VAL_293:.*]] = add i32 %tile_origin.1, 0
// CHECK:         %[[VAL_296:.*]] = load %[[VAL_1]], ptr %[[VAL_209]], align 1
// CHECK:         store %[[VAL_1]] %[[VAL_296]], ptr %[[VAL_297:.*]], align 1
// CHECK:         br label %[[VAL_202]]
// CHECK:       entry:
// CHECK:         %[[VAL_298:.*]] = alloca %[[VAL_299:.*]], align 8
// CHECK:         %[[VAL_300:.*]] = load %[[VAL_299]], ptr %[[VAL_301:.*]], align 1
// CHECK:         %[[VAL_302:.*]] = load %[[VAL_299]], ptr %[[VAL_303:.*]], align 1
// CHECK-GCN:     %[[VAL_304:.*]] = extractvalue %[[VAL_299]] %[[VAL_302]], 1
// CHECK-GCN:     %[[VAL_305:.*]] = extractvalue %[[VAL_299]] %[[VAL_300]], 1
// CHECK-PTX:     %[[VAL_304:.*]] = extractvalue %[[VAL_299]] %[[VAL_300]], 0
// CHECK-PTX:     %[[VAL_305:.*]] = extractvalue %[[VAL_299]] %[[VAL_302]], 0
// CHECK-GCN:     %[[VAL_306:.*]] = fadd double %[[VAL_305]], %[[VAL_304]] 
// CHECK-PTX:     %[[VAL_306:.*]] = fadd double %[[VAL_304]], %[[VAL_305]]
// CHECK-GCN:     %[[VAL_307:.*]] = extractvalue %[[VAL_299]] %[[VAL_302]], 0
// CHECK-GCN:     %[[VAL_308:.*]] = extractvalue %[[VAL_299]] %[[VAL_300]], 0
// CHECK-PTX:     %[[VAL_307:.*]] = extractvalue %[[VAL_299]] %[[VAL_300]], 1
// CHECK-PTX:     %[[VAL_308:.*]] = extractvalue %[[VAL_299]] %[[VAL_302]], 1
// CHECK-GCN:     %[[VAL_309:.*]] = fadd double %[[VAL_308]], %[[VAL_307]]
// CHECK-PTX:     %[[VAL_309:.*]] = fadd double %[[VAL_307]], %[[VAL_308]]
// CHECK-GCN:     %[[VAL_310:.*]] = insertvalue %[[VAL_299]] zeroinitializer, double %[[VAL_309]], 0
// CHECK-GCN:     %[[VAL_311:.*]] = insertvalue %[[VAL_299]] %[[VAL_310]], double %[[VAL_306]], 1
// CHECK-PTX:     %[[VAL_310:.*]] = insertvalue %[[VAL_299]] zeroinitializer, double %[[VAL_306]], 0
// CHECK-PTX:     %[[VAL_311:.*]] = insertvalue %[[VAL_299]] %[[VAL_310]], double %[[VAL_309]], 1
// CHECK:         store %[[VAL_299]] %[[VAL_311]], ptr{{.*}} %[[VAL_298]], align 1
// CHECK:         %[[VAL_312:.*]] = load %[[VAL_299]], ptr{{.*}} %[[VAL_298]], align 1
// CHECK:         store %[[VAL_299]] %[[VAL_312]], ptr %[[VAL_313:.*]], align 1
// CHECK:         ret void
