// RUN: hlo-opt %s --platform=gpu --stage=llvm-before-optimizations --xla_gpu_target_config_filename=%S/../../../tools/hlo_opt/gpu_specs/%{GPU}.txtpb | FileCheck %s --check-prefixes=CHECK,CHECK-%{PTX}

HloModule Test, is_scheduled=true

Add {
  scalar_lhs.0 = f32[] parameter(0)
  scalar_rhs.0 = f32[] parameter(1)
  scalar_lhs.1 = f32[] parameter(2)
  scalar_rhs.1 = f32[] parameter(3)
  add.0 = f32[] add(scalar_lhs.0, scalar_lhs.1)
  add.1 = f32[] add(scalar_rhs.0, scalar_rhs.1)
  ROOT t = (f32[], f32[]) tuple(add.0, add.1)
}

fused_computation {
  param_0 = f32[5,200,300]{2,1,0} parameter(0)
  param_1 = f32[5,200,300]{2,1,0} parameter(1)
  param_2 = f32[] parameter(2)
  ROOT d.1 = (f32[200]{0}, f32[200]{0}) reduce(f32[5,200,300]{2,1,0} param_0, f32[5,200,300]{2,1,0} %param_1, f32[] param_2, f32[] param_2), dimensions={0,2}, to_apply=Add
}

ENTRY main {
  a = f32[5, 200, 300]{2,1,0} parameter(0)
  b = f32[5, 200, 300]{2,1,0} parameter(1)
  c = f32[] constant(0)
  ROOT wrapped_d = (f32[200]{0}, f32[200]{0}) fusion(f32[5,200,300]{2,1,0} a, f32[5,200,300]{2,1,0} b, f32[] c), kind=kInput, calls=fused_computation
}

// CHECK-LABEL: entry:
// CHECK:         %[[VAL_0:.*]] = alloca float, align 4
// CHECK:         %[[VAL_1:.*]] = alloca float, align 4
// CHECK:         %[[VAL_2:.*]] = alloca float, align 4
// CHECK:         %[[VAL_3:.*]] = alloca float, align 4
// CHECK:         %[[VAL_4:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_5:.*]] = alloca float, align 4
// CHECK:         %[[VAL_6:.*]] = alloca float, align 4
// CHECK:         %[[VAL_7:.*]] = alloca float, align 4
// CHECK:         %[[VAL_8:.*]] = alloca float, align 4
// CHECK:         %[[VAL_9:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_10:.*]] = alloca float, align 4
// CHECK:         %[[VAL_11:.*]] = alloca float, align 4
// CHECK:         %[[VAL_12:.*]] = alloca float, align 4
// CHECK:         %[[VAL_13:.*]] = alloca float, align 4
// CHECK:         %[[VAL_14:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_15:.*]] = alloca float, align 4
// CHECK:         %[[VAL_16:.*]] = alloca float, align 4
// CHECK:         %[[VAL_17:.*]] = alloca float, align 4
// CHECK:         %[[VAL_18:.*]] = alloca float, align 4
// CHECK:         %[[VAL_19:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_20:.*]] = alloca float, align 4
// CHECK:         %[[VAL_21:.*]] = alloca float, align 4
// CHECK:         %[[VAL_22:.*]] = alloca float, align 4
// CHECK:         %[[VAL_23:.*]] = alloca float, align 4
// CHECK:         %[[VAL_24:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_25:.*]] = alloca float, align 4
// CHECK:         %[[VAL_26:.*]] = alloca float, align 4
// CHECK:         %[[VAL_27:.*]] = alloca float, align 4
// CHECK:         %[[VAL_28:.*]] = alloca float, align 4
// CHECK:         %[[VAL_29:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_30:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_31:.*]] = alloca float, align 4
// CHECK:         %[[VAL_32:.*]] = alloca float, align 4
// CHECK:         %[[VAL_33:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_34:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_35:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_36:.*]] = alloca i32, align 4
// CHECK:         %[[VAL_37:.*]] = alloca float, align 4
// CHECK:         %[[VAL_38:.*]] = alloca float, align 4
// CHECK:         %[[VAL_39:.*]] = alloca float, align 4
// CHECK:         %[[VAL_40:.*]] = alloca float, align 4
// CHECK-PTX:     %[[VAL_41:.*]] = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.y(), !range !2
// CHECK-GCN:     %[[VAL_41:.*]] = call i32 @llvm.amdgcn.workgroup.id.y
// CHECK:         %[[VAL_42:.*]] = icmp eq i32 %[[VAL_41]], 0
// CHECK:         br i1 %[[VAL_42]], label %[[VAL_43:.*]], label %[[VAL_44:.*]]
// CHECK:       reduce-group-0-after:                             ; preds = %thread_in_bounds-after, %[[VAL_45:.*]]
// CHECK:         ret void
// CHECK:       reduce-group-0-true:                              ; preds = %[[VAL_45]]
// CHECK:         %[[VAL_46:.*]] = load float, ptr{{.*}}%[[VAL_47:.*]], align 4, !invariant.load !{{[0-9]}}
// CHECK:         store float %[[VAL_46]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         %[[VAL_48:.*]] = load float, ptr{{.*}}%[[VAL_47]], align 4, !invariant.load !{{[0-9]}}
// CHECK:         store float %[[VAL_48]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK-PTX:     %thread.id.x = call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !4
// CHECK-GCN:     %thread.id.x = call i32 @llvm.amdgcn.workitem.id.x
// CHECK-PTX:     %block.id.x = call i32 @llvm.nvvm.read.ptx.sreg.ctaid.x(), !range !5
// CHECK-GCN:     %block.id.x = call i32 @llvm.amdgcn.workgroup.id.x
// CHECK:         %[[VAL_49:.*]] = udiv i32 %thread.id.x, 32
// CHECK:         %thread.id.1 = urem i32 %[[VAL_49]], 8
// CHECK:         %thread.id.2 = urem i32 %thread.id.x, 32
// CHECK:         %lane_id = urem i32 %thread.id.x, 32
// CHECK:         %[[VAL_50:.*]] = udiv i32 %block.id.x, 1
// CHECK:         %[[VAL_51:.*]] = urem i32 %[[VAL_50]], 1
// CHECK:         %[[VAL_52:.*]] = udiv i32 %block.id.x, 1
// CHECK:         %[[VAL_53:.*]] = urem i32 %[[VAL_52]], 25
// CHECK:         %[[VAL_54:.*]] = udiv i32 %block.id.x, 25
// CHECK:         %[[VAL_55:.*]] = icmp eq i32 %[[VAL_51]], 0
// CHECK:         %tile_bound.2 = select i1 %[[VAL_55]], i32 300, i32 512
// CHECK:         %tile_origin.0 = mul i32 %[[VAL_54]], 5
// CHECK:         %tile_origin.1 = mul i32 %[[VAL_53]], 8
// CHECK:         %tile_origin.2 = mul i32 %[[VAL_51]], 512
// CHECK:         store i32 0, ptr{{.*}}%[[VAL_36]], align 4
// CHECK:         br label %[[VAL_56:.*]]
// CHECK:       loop0.loop_header:                                ; preds = %[[VAL_57:.*]], %[[VAL_43]]
// CHECK:         %[[VAL_58:.*]] = load i32, ptr{{.*}}%[[VAL_36]], align 4
// CHECK:         %[[VAL_59:.*]] = icmp uge i32 %[[VAL_58]], 5
// CHECK:         br i1 %[[VAL_59]], label %[[VAL_60:.*]], label %[[VAL_61:.*]]
// CHECK:       loop0.loop_body:                                  ; preds = %[[VAL_56]]
// CHECK:         %[[VAL_62:.*]] = add nuw nsw i32 %[[VAL_58]], 1
// CHECK:         store i32 %[[VAL_62]], ptr{{.*}}%[[VAL_36]], align 4
// CHECK:         store i32 %thread.id.1, ptr{{.*}}%[[VAL_35]], align 4
// CHECK:         br label %[[VAL_64:.*]]
// CHECK:       loop1.loop_header:                                ; preds = %[[VAL_65:.*]], %[[VAL_61]]
// CHECK:         %[[VAL_66:.*]] = load i32, ptr{{.*}}%[[VAL_35]], align 4
// CHECK:         %[[VAL_67:.*]] = icmp uge i32 %[[VAL_66]], 8
// CHECK:         br i1 %[[VAL_67]], label %[[VAL_57]], label %[[VAL_68:.*]]
// CHECK:       loop1.loop_body:                                  ; preds = %[[VAL_64]]
// CHECK:         %[[VAL_69:.*]] = add nuw nsw i32 %[[VAL_66]], 8
// CHECK:         store i32 %[[VAL_69]], ptr{{.*}}%[[VAL_35]], align 4
// CHECK:         %[[VAL_71:.*]] = icmp eq i32 512, %tile_bound.2
// CHECK:         br i1 %[[VAL_71]], label %[[VAL_72:.*]], label %[[VAL_73:.*]]
// CHECK:       is_full_tile-after:                               ; preds = %[[VAL_74:.*]], %[[VAL_75:.*]]
// CHECK:         br label %[[VAL_64]], !llvm.loop !{{[0-9]}}
// CHECK:       loop1.loop_exit:                                  ; preds = %[[VAL_64]]
// CHECK:         br label %[[VAL_56]], !llvm.loop !{{[0-9]}}
// CHECK:       loop0.loop_exit:                                  ; preds = %[[VAL_56]]
// CHECK:         %[[VAL_76:.*]] = load float, ptr{{.*}}%[[VAL_39]], align 4
// CHECK-PTX:     %[[VAL_77:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_76]], i32 16, i32 31)
// CHECK-GCN:     %[[VAL_76_1:.*]] = bitcast float %[[VAL_76]] to i32
// CHECK-GCN:     %[[VAL_77_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_76_1:.*]], i32 543)
// CHECK-GCN:     %[[VAL_77:.*]] = bitcast i32 %[[VAL_77_1:.*]] to float
// CHECK:         store float %[[VAL_77]], ptr{{.*}}%[[VAL_26]], align 4
// CHECK:         %[[VAL_78:.*]] = load float, ptr{{.*}}%[[VAL_37]], align 4
// CHECK-PTX:     %[[VAL_79:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_78]], i32 16, i32 31)
// CHECK-GCN:     %[[VAL_78_1:.*]] = bitcast float %[[VAL_78]] to i32
// CHECK-GCN:     %[[VAL_79_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_78_1:.*]], i32 543)
// CHECK-GCN:     %[[VAL_79:.*]] = bitcast i32 %[[VAL_79_1:.*]] to float
// CHECK:         store float %[[VAL_79]], ptr{{.*}}%[[VAL_25]], align 4
// CHECK-GCN:     %[[VAL_22_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_22]] to ptr
// CHECK:         %[[VAL_80:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_24]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_22]], ptr %[[VAL_80]], align 8
// CHECK-GCN:     store ptr %[[VAL_22_1]], ptr{{.*}}%[[VAL_80]], align 8
// CHECK-GCN:     %[[VAL_23_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_23]] to ptr
// CHECK:         %[[VAL_81:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_24]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_23]], ptr %[[VAL_81]], align 8
// CHECK-GCN:     store ptr %[[VAL_23_1]], ptr{{.*}}%[[VAL_81]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_26]], ptr %[[VAL_25]], ptr %[[VAL_24]])
// CHECK-GCN:     %[[VAL_39_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_26_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_26]] to ptr
// CHECK-GCN:     %[[VAL_25_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_25]] to ptr
// CHECK-GCN:     %[[VAL_24_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_24]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_1]], ptr %[[VAL_37_1]], ptr %[[VAL_26_1]], ptr %[[VAL_25_1]], ptr %[[VAL_24_1]])
// CHECK:         %[[VAL_82:.*]] = load float, ptr{{.*}}%[[VAL_22]], align 4
// CHECK:         %[[VAL_83:.*]] = load float, ptr{{.*}}%[[VAL_23]], align 4
// CHECK:         store float %[[VAL_82]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_83]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         %[[VAL_84:.*]] = load float, ptr{{.*}}%[[VAL_39]], align 4
// CHECK-PTX:     %[[VAL_85:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_84]], i32 8, i32 31)
// CHECK-GCN:     %[[VAL_84_1:.*]] = bitcast float %[[VAL_84]] to i32
// CHECK-GCN:     %[[VAL_85_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_84_1:.*]], i32 287)
// CHECK-GCN:     %[[VAL_85:.*]] = bitcast i32 %[[VAL_85_1:.*]] to float
// CHECK:         store float %[[VAL_85]], ptr{{.*}}%[[VAL_21]], align 4
// CHECK:         %[[VAL_86:.*]] = load float, ptr{{.*}}%[[VAL_37]], align 4
// CHECK-PTX:     %[[VAL_87:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_86]], i32 8, i32 31)
// CHECK-GCN:     %[[VAL_86_1:.*]] = bitcast float %[[VAL_86]] to i32
// CHECK-GCN:     %[[VAL_87_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_86_1:.*]], i32 287)
// CHECK-GCN:     %[[VAL_87:.*]] = bitcast i32 %[[VAL_87_1:.*]] to float
// CHECK:         store float %[[VAL_87]], ptr{{.*}}%[[VAL_20]], align 4
// CHECK-GCN:     %[[VAL_17_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_17]] to ptr
// CHECK:         %[[VAL_88:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_19]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_17]], ptr %[[VAL_88]], align 8
// CHECK-GCN:     store ptr %[[VAL_17_1]], ptr{{.*}}%[[VAL_88]], align 8
// CHECK-GCN:     %[[VAL_18_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_18]] to ptr
// CHECK:         %[[VAL_89:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_19]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_18]], ptr %[[VAL_89]], align 8
// CHECK-GCN:     store ptr %[[VAL_18_1]], ptr{{.*}}%[[VAL_89]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_21]], ptr %[[VAL_20]], ptr %[[VAL_19]])
// CHECK-GCN:     %[[VAL_39_2:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_2:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_21_2:.*]] = addrspacecast ptr{{.*}}%[[VAL_21]] to ptr
// CHECK-GCN:     %[[VAL_20_2:.*]] = addrspacecast ptr{{.*}}%[[VAL_20]] to ptr
// CHECK-GCN:     %[[VAL_19_2:.*]] = addrspacecast ptr{{.*}}%[[VAL_19]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_2]], ptr %[[VAL_37_2]], ptr %[[VAL_21_2]], ptr %[[VAL_20_2]], ptr %[[VAL_19_2]])
// CHECK:         %[[VAL_90:.*]] = load float, ptr{{.*}}%[[VAL_17]], align 4
// CHECK:         %[[VAL_91:.*]] = load float, ptr{{.*}}%[[VAL_18]], align 4
// CHECK:         store float %[[VAL_90]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_91]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         %[[VAL_92:.*]] = load float, ptr{{.*}}%[[VAL_39]], align 4
// CHECK-PTX:     %[[VAL_93:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_92]], i32 4, i32 31)
// CHECK-GCN:     %[[VAL_92_1:.*]] = bitcast float %[[VAL_92]] to i32
// CHECK-GCN:     %[[VAL_93_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_92_1:.*]], i32 159)
// CHECK-GCN:     %[[VAL_93:.*]] = bitcast i32 %[[VAL_93_1:.*]] to float
// CHECK:         store float %[[VAL_93]], ptr{{.*}}%[[VAL_16]], align 4
// CHECK:         %[[VAL_94:.*]] = load float, ptr{{.*}}%[[VAL_37]], align 4
// CHECK-PTX:     %[[VAL_95:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_94]], i32 4, i32 31)
// CHECK-GCN:     %[[VAL_94_1:.*]] = bitcast float %[[VAL_94]] to i32
// CHECK-GCN:     %[[VAL_95_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_94_1:.*]], i32 159)
// CHECK-GCN:     %[[VAL_95:.*]] = bitcast i32 %[[VAL_95_1:.*]] to float
// CHECK:         store float %[[VAL_95]], ptr{{.*}}%[[VAL_15]], align 4
// CHECK-GCN:     %[[VAL_12_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_12]] to ptr
// CHECK:         %[[VAL_96:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_14]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_12]], ptr %[[VAL_96]], align 8
// CHECK-GCN:     store ptr %[[VAL_12_1]], ptr{{.*}}%[[VAL_96]], align 8
// CHECK-GCN:     %[[VAL_13_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_13]] to ptr
// CHECK:         %[[VAL_97:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_14]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_13]], ptr %[[VAL_97]], align 8
// CHECK-GCN:     store ptr %[[VAL_13_1]], ptr{{.*}}%[[VAL_97]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_16]], ptr %[[VAL_15]], ptr %[[VAL_14]])
// CHECK-GCN:     %[[VAL_39_3:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_3:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_16_3:.*]] = addrspacecast ptr{{.*}}%[[VAL_16]] to ptr
// CHECK-GCN:     %[[VAL_15_3:.*]] = addrspacecast ptr{{.*}}%[[VAL_15]] to ptr
// CHECK-GCN:     %[[VAL_14_3:.*]] = addrspacecast ptr{{.*}}%[[VAL_14]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_3]], ptr %[[VAL_37_3]], ptr %[[VAL_16_3]], ptr %[[VAL_15_3]], ptr %[[VAL_14_3]])
// CHECK:         %[[VAL_98:.*]] = load float, ptr{{.*}}%[[VAL_12]], align 4
// CHECK:         %[[VAL_99:.*]] = load float, ptr{{.*}}%[[VAL_13]], align 4
// CHECK:         store float %[[VAL_98]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_99]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         %[[VAL_100:.*]] = load float, ptr{{.*}}%[[VAL_39]], align 4
// CHECK-PTX:     %[[VAL_101:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_100]], i32 2, i32 31)
// CHECK-GCN:     %[[VAL_100_1:.*]] = bitcast float %[[VAL_100]] to i32
// CHECK-GCN:     %[[VAL_101_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_100_1:.*]], i32 95)
// CHECK-GCN:     %[[VAL_101:.*]] = bitcast i32 %[[VAL_101_1:.*]] to float
// CHECK:         store float %[[VAL_101]], ptr{{.*}}%[[VAL_11]], align 4
// CHECK:         %[[VAL_102:.*]] = load float, ptr{{.*}}%[[VAL_37]], align 4
// CHECK-PTX:     %[[VAL_103:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_102]], i32 2, i32 31)
// CHECK-GCN:     %[[VAL_102_1:.*]] = bitcast float %[[VAL_102]] to i32
// CHECK-GCN:     %[[VAL_103_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_102_1:.*]], i32 95)
// CHECK-GCN:     %[[VAL_103:.*]] = bitcast i32 %[[VAL_103_1:.*]] to float
// CHECK:         store float %[[VAL_103]], ptr{{.*}}%[[VAL_10]], align 4
// CHECK-GCN:     %[[VAL_7_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_7]] to ptr
// CHECK:         %[[VAL_104:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_9]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_7]], ptr %[[VAL_104]], align 8
// CHECK-GCN:     store ptr %[[VAL_7_1]], ptr{{.*}}%[[VAL_104]], align 8
// CHECK-GCN:     %[[VAL_8_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_8]] to ptr
// CHECK:         %[[VAL_105:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_9]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_8]], ptr %[[VAL_105]], align 8
// CHECK-GCN:     store ptr %[[VAL_8_1]], ptr{{.*}}%[[VAL_105]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_11]], ptr %[[VAL_10]], ptr %[[VAL_9]])
// CHECK-GCN:     %[[VAL_39_4:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_4:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_11_4:.*]] = addrspacecast ptr{{.*}}%[[VAL_11]] to ptr
// CHECK-GCN:     %[[VAL_10_4:.*]] = addrspacecast ptr{{.*}}%[[VAL_10]] to ptr
// CHECK-GCN:     %[[VAL_9_4:.*]] = addrspacecast ptr{{.*}}%[[VAL_9]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_4]], ptr %[[VAL_37_4]], ptr %[[VAL_11_4]], ptr %[[VAL_10_4]], ptr %[[VAL_9_4]])
// CHECK:         %[[VAL_106:.*]] = load float, ptr{{.*}}%[[VAL_7]], align 4
// CHECK:         %[[VAL_107:.*]] = load float, ptr{{.*}}%[[VAL_8]], align 4
// CHECK:         store float %[[VAL_106]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_107]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         %[[VAL_108:.*]] = load float, ptr{{.*}}%[[VAL_39]], align 4
// CHECK-PTX:     %[[VAL_109:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_108]], i32 1, i32 31)
// CHECK-GCN:     %[[VAL_108_1:.*]] = bitcast float %[[VAL_108]] to i32
// CHECK-GCN:     %[[VAL_109_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_108_1:.*]], i32 63)
// CHECK-GCN:     %[[VAL_109:.*]] = bitcast i32 %[[VAL_109_1:.*]] to float
// CHECK:         store float %[[VAL_109]], ptr{{.*}}%[[VAL_6]], align 4
// CHECK:         %[[VAL_110:.*]] = load float, ptr{{.*}}%[[VAL_37]], align 4
// CHECK-PTX:     %[[VAL_111:.*]] = call float @llvm.nvvm.shfl.sync.down.f32(i32 -1, float %[[VAL_110]], i32 1, i32 31)
// CHECK-GCN:     %[[VAL_110_1:.*]] = bitcast float %[[VAL_110]] to i32
// CHECK-GCN:     %[[VAL_111_1:.*]] = call i32 @llvm.amdgcn.ds.swizzle(i32 %[[VAL_110_1:.*]], i32 63)
// CHECK-GCN:     %[[VAL_111:.*]] = bitcast i32 %[[VAL_111_1:.*]] to float
// CHECK:         store float %[[VAL_111]], ptr{{.*}}%[[VAL_5]], align 4
// CHECK-GCN:     %[[VAL_2_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_2]] to ptr
// CHECK:         %[[VAL_112:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_4]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_2]], ptr %[[VAL_112]], align 8
// CHECK-GCN:     store ptr %[[VAL_2_1]], ptr{{.*}}%[[VAL_112]], align 8
// CHECK-GCN:     %[[VAL_3_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_3]] to ptr
// CHECK:         %[[VAL_113:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_4]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_3]], ptr %[[VAL_113]], align 8
// CHECK-GCN:     store ptr %[[VAL_3_1]], ptr{{.*}}%[[VAL_113]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_6]], ptr %[[VAL_5]], ptr %[[VAL_4]])
// CHECK-GCN:     %[[VAL_39_5:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_5:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_6_5:.*]] = addrspacecast ptr{{.*}}%[[VAL_6]] to ptr
// CHECK-GCN:     %[[VAL_5_5:.*]] = addrspacecast ptr{{.*}}%[[VAL_5]] to ptr
// CHECK-GCN:     %[[VAL_4_5:.*]] = addrspacecast ptr{{.*}}%[[VAL_4]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_5]], ptr %[[VAL_37_5]], ptr %[[VAL_6_5]], ptr %[[VAL_5_5]], ptr %[[VAL_4_5]])
// CHECK:         %[[VAL_114:.*]] = load float, ptr{{.*}}%[[VAL_2]], align 4
// CHECK:         %[[VAL_115:.*]] = load float, ptr{{.*}}%[[VAL_3]], align 4
// CHECK:         store float %[[VAL_114]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_115]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         %[[VAL_116:.*]] = udiv i32 %thread.id.2, 32
// CHECK:         %[[VAL_117:.*]] = icmp ult i32 %thread.id.1, 8
// CHECK:         br i1 %[[VAL_117]], label %thread_in_bounds-true, label %thread_in_bounds-after
// CHECK:       thread_in_bounds-after:                           ; preds = %[[VAL_118:.*]], %[[VAL_60]]
// CHECK:         br label %[[VAL_44]]
// CHECK:       is_full_tile-true:                                ; preds = %[[VAL_68]]
// CHECK:         store i32 0, ptr{{.*}}%[[VAL_34]], align 4
// CHECK:         br label %[[VAL_119:.*]]
// CHECK:       loop2.loop_header:                                ; preds = %[[VAL_120:.*]], %[[VAL_72]]
// CHECK:         %[[VAL_121:.*]] = load i32, ptr{{.*}}%[[VAL_34]], align 4
// CHECK:         %[[VAL_122:.*]] = icmp uge i32 %[[VAL_121]], 512
// CHECK:         br i1 %[[VAL_122]], label %[[VAL_75]], label %[[VAL_120]]
// CHECK:       loop2.loop_body:                                  ; preds = %[[VAL_119]]
// CHECK:         %[[VAL_123:.*]] = add nuw nsw i32 %[[VAL_121]], 32
// CHECK:         store i32 %[[VAL_123]], ptr{{.*}}%[[VAL_34]], align 4
// CHECK:         %[[VAL_125:.*]] = add i32 %[[VAL_121]], %thread.id.2
// CHECK:         %[[VAL_126:.*]] = add i32 %tile_origin.0, %[[VAL_58]]
// CHECK:         %[[VAL_127:.*]] = add i32 %tile_origin.1, %[[VAL_66]]
// CHECK:         %[[VAL_128:.*]] = add i32 %tile_origin.2, %[[VAL_125]]
// CHECK:         %[[VAL_129:.*]] = getelementptr inbounds [5 x [200 x [300 x float]]], ptr{{.*}}%[[VAL_130:.*]], i32 0, i32 %[[VAL_126]], i32 %[[VAL_127]], i32 %[[VAL_128]]
// CHECK:         %[[VAL_131:.*]] = load float, ptr{{.*}}%[[VAL_129]], align 4, !invariant.load !{{[0-9]}}
// CHECK:         store float %[[VAL_131]], ptr{{.*}}%[[VAL_40]], align 4
// CHECK:         %[[VAL_132:.*]] = getelementptr inbounds [5 x [200 x [300 x float]]], ptr{{.*}}%[[VAL_133:.*]], i32 0, i32 %[[VAL_126]], i32 %[[VAL_127]], i32 %[[VAL_128]]
// CHECK:         %[[VAL_134:.*]] = load float, ptr{{.*}}%[[VAL_132]], align 4, !invariant.load !{{[0-9]}}
// CHECK:         store float %[[VAL_134]], ptr{{.*}}%[[VAL_38]], align 4
// CHECK-GCN:     %[[VAL_31_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_31]] to ptr
// CHECK:         %[[VAL_135:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_33]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_31]], ptr %[[VAL_135]], align 8
// CHECK-GCN:     store ptr %[[VAL_31_1]], ptr{{.*}}%[[VAL_135]], align 8
// CHECK-GCN:     %[[VAL_32_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_32]] to ptr
// CHECK:         %[[VAL_136:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_33]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_32]], ptr %[[VAL_136]], align 8
// CHECK-GCN:     store ptr %[[VAL_32_1]], ptr{{.*}}%[[VAL_136]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_40]], ptr %[[VAL_38]], ptr %[[VAL_33]])
// CHECK-GCN:     %[[VAL_39_6:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_6:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_40_6:.*]] = addrspacecast ptr{{.*}}%[[VAL_40]] to ptr
// CHECK-GCN:     %[[VAL_38_6:.*]] = addrspacecast ptr{{.*}}%[[VAL_38]] to ptr
// CHECK-GCN:     %[[VAL_33_6:.*]] = addrspacecast ptr{{.*}}%[[VAL_33]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_6]], ptr %[[VAL_37_6]], ptr %[[VAL_40_6]], ptr %[[VAL_38_6]], ptr %[[VAL_33_6]])
// CHECK:         %[[VAL_137:.*]] = load float, ptr{{.*}}%[[VAL_31]], align 4
// CHECK:         %[[VAL_138:.*]] = load float, ptr{{.*}}%[[VAL_32]], align 4
// CHECK:         store float %[[VAL_137]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_138]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         br label %[[VAL_119]], !llvm.loop !{{[0-9]}}
// CHECK:       loop2.loop_exit:                                  ; preds = %[[VAL_119]]
// CHECK:         br label %[[VAL_65]]
// CHECK:       is_full_tile-false:                               ; preds = %[[VAL_68]]
// CHECK:         store i32 0, ptr{{.*}}%[[VAL_30]], align 4
// CHECK:         br label %[[VAL_139:.*]]
// CHECK:       loop2.loop_header9:                               ; preds = %[[VAL_140:.*]], %[[VAL_73]]
// CHECK:         %[[VAL_141:.*]] = load i32, ptr{{.*}}%[[VAL_30]], align 4
// CHECK:         %[[VAL_142:.*]] = icmp uge i32 %[[VAL_141]], 512
// CHECK:         br i1 %[[VAL_142]], label %[[VAL_74]], label %[[VAL_143:.*]]
// CHECK:       loop2.loop_body10:                                ; preds = %[[VAL_139]]
// CHECK:         %[[VAL_144:.*]] = add nuw nsw i32 %[[VAL_141]], 32
// CHECK:         store i32 %[[VAL_144]], ptr{{.*}}%[[VAL_30]], align 4
// CHECK:         %[[VAL_146:.*]] = add i32 %[[VAL_141]], %thread.id.2
// CHECK:         %[[VAL_147:.*]] = icmp ult i32 %[[VAL_146]], %tile_bound.2
// CHECK:         br i1 %[[VAL_147]], label %[[VAL_148:.*]], label %[[VAL_140]]
// CHECK:       x_in_tile-after:                                  ; preds = %[[VAL_148]], %[[VAL_143]]
// CHECK:         br label %[[VAL_139]], !llvm.loop !{{[0-9]}}
// CHECK:       loop2.loop_exit8:                                 ; preds = %[[VAL_139]]
// CHECK:         br label %[[VAL_65]]
// CHECK:       x_in_tile-true:                                   ; preds = %[[VAL_143]]
// CHECK:         %[[VAL_149:.*]] = add i32 %tile_origin.0, %[[VAL_58]]
// CHECK:         %[[VAL_150:.*]] = add i32 %tile_origin.1, %[[VAL_66]]
// CHECK:         %[[VAL_151:.*]] = add i32 %tile_origin.2, %[[VAL_146]]
// CHECK:         %[[VAL_152:.*]] = getelementptr inbounds [5 x [200 x [300 x float]]], ptr{{.*}}%[[VAL_130]], i32 0, i32 %[[VAL_149]], i32 %[[VAL_150]], i32 %[[VAL_151]]
// CHECK:         %[[VAL_153:.*]] = load float, ptr{{.*}}%[[VAL_152]], align 4, !invariant.load !{{[0-9]}}
// CHECK:         store float %[[VAL_153]], ptr{{.*}}%[[VAL_40]], align 4
// CHECK:         %[[VAL_154:.*]] = getelementptr inbounds [5 x [200 x [300 x float]]], ptr{{.*}}%[[VAL_133]], i32 0, i32 %[[VAL_149]], i32 %[[VAL_150]], i32 %[[VAL_151]]
// CHECK:         %[[VAL_155:.*]] = load float, ptr{{.*}}%[[VAL_154]], align 4, !invariant.load !{{[0-9]}}
// CHECK:         store float %[[VAL_155]], ptr{{.*}}%[[VAL_38]], align 4
// CHECK-GCN:     %[[VAL_27_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_27]] to ptr
// CHECK:         %[[VAL_156:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_29]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_27]], ptr %[[VAL_156]], align 8
// CHECK-GCN:     store ptr %[[VAL_27_1]], ptr{{.*}}%[[VAL_156]], align 8
// CHECK-GCN:     %[[VAL_28_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_28]] to ptr
// CHECK:         %[[VAL_157:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_29]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_28]], ptr %[[VAL_157]], align 8
// CHECK-GCN:     store ptr %[[VAL_28_1]], ptr{{.*}}%[[VAL_157]], align 8
// CHECK-PTX:     call void @[[ADD:Add.*]](ptr %[[VAL_39]], ptr %[[VAL_37]], ptr %[[VAL_40]], ptr %[[VAL_38]], ptr %[[VAL_29]])
// CHECK-GCN:     %[[VAL_39_7:.*]] = addrspacecast ptr{{.*}}%[[VAL_39]] to ptr
// CHECK-GCN:     %[[VAL_37_7:.*]] = addrspacecast ptr{{.*}}%[[VAL_37]] to ptr
// CHECK-GCN:     %[[VAL_40_7:.*]] = addrspacecast ptr{{.*}}%[[VAL_40]] to ptr
// CHECK-GCN:     %[[VAL_38_7:.*]] = addrspacecast ptr{{.*}}%[[VAL_38]] to ptr
// CHECK-GCN:     %[[VAL_29_7:.*]] = addrspacecast ptr{{.*}}%[[VAL_29]] to ptr
// CHECK-GCN:     call void @[[ADD:Add.*]](ptr %[[VAL_39_7]], ptr %[[VAL_37_7]], ptr %[[VAL_40_7]], ptr %[[VAL_38_7]], ptr %[[VAL_29_7]])
// CHECK:         %[[VAL_158:.*]] = load float, ptr{{.*}}%[[VAL_27]], align 4
// CHECK:         %[[VAL_159:.*]] = load float, ptr{{.*}}%[[VAL_28]], align 4
// CHECK:         store float %[[VAL_158]], ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         store float %[[VAL_159]], ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         br label %[[VAL_140]]
// CHECK:       thread_in_bounds-true:                            ; preds = %[[VAL_60]]
// CHECK:         %[[VAL_160:.*]] = icmp eq i32 %lane_id, 0
// CHECK:         br i1 %[[VAL_160]], label %[[VAL_161:.*]], label %[[VAL_162:.*]]
// CHECK:       intra_warp_reduce_write-after:                    ; preds = %[[VAL_161]], %thread_in_bounds-true
// CHECK-PTX:     call void @llvm.nvvm.barrier0()
// CHECK-GCN:     call void @llvm.amdgcn.s.barrier()
// CHECK:         %[[VAL_163:.*]] = icmp eq i32 %[[VAL_116]], 0
// CHECK:         br i1 %[[VAL_163]], label %[[VAL_164:.*]], label %[[VAL_118]]
// CHECK:       inter_warp_reduce-after:                          ; preds = %[[VAL_165:.*]], %[[VAL_162]]
// CHECK:         br label %thread_in_bounds-after
// CHECK:       intra_warp_reduce_write-true:                     ; preds = %thread_in_bounds-true
// CHECK:         %[[VAL_166:.*]] = load float, ptr{{.*}}%[[VAL_39]], align 4
// CHECK:         %[[VAL_167:.*]] = getelementptr inbounds [8 x [1 x float]], ptr addrspace(3) @shared_cache, i32 0, i32 %thread.id.1, i32 %[[VAL_116]]
// CHECK:         %[[VAL_168:.*]] = addrspacecast ptr addrspace(3) %[[VAL_167]] to ptr
// CHECK:         store float %[[VAL_166]], ptr{{.*}}%[[VAL_168]], align 4
// CHECK:         %[[VAL_169:.*]] = load float, ptr{{.*}}%[[VAL_37]], align 4
// CHECK:         %[[VAL_170:.*]] = getelementptr inbounds [8 x [1 x float]], ptr addrspace(3) @shared_cache{{.*}}, i32 0, i32 %thread.id.1, i32 %[[VAL_116]]
// CHECK:         %[[VAL_171:.*]] = addrspacecast ptr addrspace(3) %[[VAL_170]] to ptr
// CHECK:         store float %[[VAL_169]], ptr{{.*}}%[[VAL_171]], align 4
// CHECK:         br label %[[VAL_162]]
// CHECK:       inter_warp_reduce-true:                           ; preds = %[[VAL_162]]
// CHECK:         %[[VAL_172:.*]] = getelementptr inbounds [8 x [1 x float]], ptr addrspace(3) @shared_cache, i32 0, i32 %thread.id.1, i32 %lane_id
// CHECK:         %[[VAL_173:.*]] = addrspacecast ptr addrspace(3) %[[VAL_172]] to ptr
// CHECK-GCN:     %[[VAL_1_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_1]] to ptr
// CHECK-PTX:     store float %[[VAL_46]], ptr %[[VAL_1]], align 4
// CHECK-GCN:     store float %[[VAL_46]], ptr %[[VAL_1_1]], align 4
// CHECK:         %[[VAL_174:.*]] = icmp ult i32 %thread.id.2, 1
// CHECK-PTX:     %[[VAL_175:.*]] = select i1 %[[VAL_174]], ptr %[[VAL_173]], ptr %[[VAL_1]]
// CHECK-GCN:     %[[VAL_175:.*]] = select i1 %[[VAL_174]], ptr %[[VAL_173]], ptr %[[VAL_1_1]]
// CHECK:         %[[VAL_176:.*]] = getelementptr inbounds [8 x [1 x float]], ptr addrspace(3) @shared_cache{{.*}}, i32 0, i32 %thread.id.1, i32 %lane_id
// CHECK:         %[[VAL_177:.*]] = addrspacecast ptr addrspace(3) %[[VAL_176]] to ptr
// CHECK-GCN:     %[[VAL_0_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_0]] to ptr
// CHECK-PTX:     store float %[[VAL_48]], ptr{{.*}}%[[VAL_0]], align 4
// CHECK-GCN:     store float %[[VAL_48]], ptr{{.*}}%[[VAL_0_1]], align 4
// CHECK:         %[[VAL_178:.*]] = icmp ult i32 %thread.id.2, 1
// CHECK-PTX:     %[[VAL_179:.*]] = select i1 %[[VAL_178]], ptr{{.*}}%[[VAL_177]], ptr %[[VAL_0]]
// CHECK-GCN:     %[[VAL_179:.*]] = select i1 %[[VAL_178]], ptr{{.*}}%[[VAL_177]], ptr %[[VAL_0_1]]
// CHECK:         %[[VAL_180:.*]] = icmp eq i32 %thread.id.2, 0
// CHECK:         br i1 %[[VAL_180]], label %[[VAL_181:.*]], label %[[VAL_165]]
// CHECK:       reduction_write_output-after:                     ; preds = %[[VAL_181]], %[[VAL_164]]
// CHECK:         br label %[[VAL_118]]
// CHECK:       reduction_write_output-true:                      ; preds = %[[VAL_164]]
// CHECK:         %[[VAL_183:.*]] = add i32 %tile_origin.1, %thread.id.1
// CHECK:         %[[VAL_186:.*]] = getelementptr inbounds [200 x float], ptr{{.*}}%[[VAL_187:.*]], i32 0, i32 %[[VAL_183]]
// CHECK:         %[[VAL_188:.*]] = load float, ptr{{.*}}%[[VAL_175]], align 4
// CHECK:         store float %[[VAL_188]], ptr{{.*}}%[[VAL_186]], align 4
// CHECK:         %[[VAL_190:.*]] = add i32 %tile_origin.1, %thread.id.1
// CHECK:         %[[VAL_193:.*]] = getelementptr inbounds [200 x float], ptr{{.*}}%[[VAL_194:.*]], i32 0, i32 %[[VAL_190]]
// CHECK:         %[[VAL_195:.*]] = load float, ptr{{.*}}%[[VAL_179]], align 4
// CHECK:         store float %[[VAL_195]], ptr{{.*}}%[[VAL_193]], align 4
// CHECK:         br label %[[VAL_165]]
// CHECK:       entry:
// CHECK:         %[[VAL_196:.*]] = alloca float, align 4
// CHECK:         %[[VAL_197:.*]] = alloca float, align 4
// CHECK:         %[[VAL_198:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_199:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_200:.*]] = alloca [2 x ptr], align 8
// CHECK:         %[[VAL_201:.*]] = load float, ptr{{.*}}%[[VAL_202:.*]], align 4
// CHECK:         %[[VAL_203:.*]] = load float, ptr{{.*}}%[[VAL_204:.*]], align 4
// CHECK:         %[[VAL_205:.*]] = fadd float %[[VAL_201]], %[[VAL_203]]
// CHECK:         store float %[[VAL_205]], ptr{{.*}}%[[VAL_197]], align 4
// CHECK:         %[[VAL_206:.*]] = load float, ptr{{.*}}%[[VAL_207:.*]], align 4
// CHECK:         %[[VAL_208:.*]] = load float, ptr{{.*}}%[[VAL_209:.*]], align 4
// CHECK:         %[[VAL_210:.*]] = fadd float %[[VAL_206]], %[[VAL_208]]
// CHECK:         store float %[[VAL_210]], ptr{{.*}}%[[VAL_196]], align 4
// CHECK-GCN:     %[[VAL_197_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_197]] to ptr
// CHECK:         %[[VAL_211:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_200]], i64 0, i64 0
// CHECK-PTX:     store ptr %[[VAL_197]], ptr %[[VAL_211]], align 8
// CHECK-GCN:     store ptr %[[VAL_197_1]], ptr{{.*}}%[[VAL_211]], align 8
// CHECK-GCN:     %[[VAL_196_1:.*]] = addrspacecast ptr{{.*}}%[[VAL_196]] to ptr
// CHECK:         %[[VAL_212:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_200]], i64 0, i64 1
// CHECK-PTX:     store ptr %[[VAL_196]], ptr %[[VAL_212]], align 8
// CHECK-GCN:     store ptr %[[VAL_196_1]], ptr{{.*}}%[[VAL_212]], align 8
// CHECK:         %[[VAL_213:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_214:.*]], i64 0, i64 0
// CHECK:         %[[VAL_215:.*]] = load ptr, ptr{{.*}}%[[VAL_213]], align 8, !dereferenceable !{{[0-9]*}}, !align !{{[0-9]*}}
// CHECK:         %[[VAL_216:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_200]], i64 0, i64 0
// CHECK:         %[[VAL_217:.*]] = load ptr, ptr{{.*}}%[[VAL_216]], align 8, !dereferenceable !{{[0-9]*}}, !align !{{[0-9]*}}
// CHECK:         %[[VAL_218:.*]] = load float, ptr{{.*}}%[[VAL_217]], align 4
// CHECK:         store float %[[VAL_218]], ptr{{.*}}%[[VAL_215]], align 4
// CHECK:         %[[VAL_219:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_214]], i64 0, i64 1
// CHECK:         %[[VAL_220:.*]] = load ptr, ptr{{.*}}%[[VAL_219]], align 8, !dereferenceable !{{[0-9]*}}, !align !{{[0-9]*}}
// CHECK:         %[[VAL_221:.*]] = getelementptr inbounds [2 x ptr], ptr{{.*}}%[[VAL_200]], i64 0, i64 1
// CHECK:         %[[VAL_222:.*]] = load ptr, ptr{{.*}}%[[VAL_221]], align 8, !dereferenceable !{{[0-9]*}}, !align !{{[0-9]*}}
// CHECK:         %[[VAL_223:.*]] = load float, ptr{{.*}}%[[VAL_222]], align 4
// CHECK:         store float %[[VAL_223]], ptr{{.*}}%[[VAL_220]], align 4
// CHECK:         ret void

