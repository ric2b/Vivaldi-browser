// Copyright 2023 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

module webnn.mojom;

import "mojo/public/mojom/base/unguessable_token.mojom";
import "services/webnn/public/mojom/features.mojom";
import "services/webnn/public/mojom/webnn_buffer.mojom";
import "services/webnn/public/mojom/webnn_error.mojom";
import "services/webnn/public/mojom/webnn_graph.mojom";

// Represents options of creating `WebNNContext` interface.
struct CreateContextOptions {
  enum Device {
    // Provides the broadest compatibility and usability across all client
    // devices with varying degrees of performance.
    kCpu,
    // Provides the broadest range of achievable performance across graphics
    // hardware platforms from consumer devices to professional workstations.
    kGpu,
    // NPU (neural processing unit) is a class of specialized hardware
    // accelerator designed to accelerate artificial intelligence and machine
    // learning applications. Unlike more general-purpose devices such as the
    // GPU and CPU, an NPU supports a limited finite set of operations and may
    // not have programmability support. The fallback behavior is being
    // discussed by WG at:
    // https://github.com/webmachinelearning/webnn/issues/623.
    // This Enum value is introduced for testing purpose that will inform
    // the WG.
    kNpu,
  };

  enum PowerPreference {
    // Let the user agent select the most suitable behavior.
    kDefault,
    // Prioritizes execution speed over power consumption.
    kHighPerformance,
    // Prioritizes power consumption over other considerations such as execution
    // speed.
    kLowPower,
  };

  // Indicates the kind of device used for the context.
  Device device;
  // The power preference for power consumption.
  PowerPreference power_preference;

  // Hint for the number of threads to use for inference, with zero indicating a
  // preference to defer this decision to the backend. A backend may also choose
  // to ignore this value entirely.
  //
  // TODO(crbug.com/338162119): This is not the final expected API shape. Spec
  // discussion on how this information should be communicated is happening
  // here: https://github.com/webmachinelearning/webnn/issues/436
  uint8 thread_count_hint;
};

struct SupportedDataTypes {
  bool float32;
  bool float16;
  bool int32;
  bool uint32;
  bool int64;
  bool uint64;
  bool int8;
  bool uint8;
};

// Represents the data type limits exposed through
// `MLContext::opSupportLimits()`.
// Keep the order as the same as build methods of `MLGraphBuilder`, for
// operator with multiple parameters, maintain the parameter positions order.
struct DataTypeLimits {
  SupportedDataTypes input;
  SupportedDataTypes constant;
  SupportedDataTypes arg_min_max_input;
  SupportedDataTypes arg_min_max_output;
  SupportedDataTypes concat_inputs;
  SupportedDataTypes gather_input;
  SupportedDataTypes gather_indices;
  SupportedDataTypes where_condition;
  SupportedDataTypes where_true_value;
  SupportedDataTypes where_false_value;
};

// Represents properties of the `WebNNContext` implementation that has been
// provided.
struct ContextProperties {
  InputOperandLayout input_operand_layout;
  DataTypeLimits data_type_limits;
};

// Represents a successful call to `WebNNContextProvider::CreateWebNNContext`.
struct CreateContextSuccess {
  pending_remote<WebNNContext>? context_remote;
  pending_receiver<WebNNContextClient>? context_client_receiver;
  ContextProperties context_properties;
  // context_handle is a generated token used as handle to identify the
  // the context from the renderer. The token is only valid for the lifetime
  // of the context and is used to resolve a buffer in the service using the
  // context corresponding to this handle.
  mojo_base.mojom.UnguessableToken context_handle;
};

// Represents the return value of `WebNNContext::CreateGraph()`. Let it be
// `graph_remote` if the graph was successfully created and `error` otherwise.
union CreateGraphResult {
  pending_associated_remote<WebNNGraph>? graph_remote;
  Error error;
};

// Represents the return value of `WebNNContextProvider::CreateWebNNContext()`.
// Let it be `success` if a WebNNContext was successfully created and `error`
// otherwise.
union CreateContextResult {
  CreateContextSuccess success;
  Error error;
};

// Represents the `MLContext` object in the WebIDL definition that is a global
// state of neural network compute workload and execution processes. This
// interface runs in the GPU process and is called from the renderer process.
[RuntimeFeature=webnn.mojom.features.kWebMachineLearningNeuralNetwork]
interface WebNNContext {
  // Called by the renderer process to create `WebNNGraph` message pipe for
  // executing computational graph, the WebNN graph will be validated and
  // compiled. Initializes the compiled graph for optimal performance of the
  // subsequent graph executions if the initialization is a necessary step.
  CreateGraph(GraphInfo graph_info) => (CreateGraphResult result);

  // Called by the renderer process to create `WebNNBuffer` message pipe for
  // creating platform specific buffers, the WebNN buffer will be validated and
  // created. This method guarantees memory allocation on the device.
  // The receiver represents the connection to the MLBuffer and the
  // buffer_handle is a generated token used as a handle to identify the buffer
  // from the renderer.
  CreateBuffer(pending_associated_receiver<WebNNBuffer> receiver,
               BufferInfo buffer_info,
               mojo_base.mojom.UnguessableToken buffer_handle);
};

// This interface runs in the renderer process and is called from the GPU
// process. When the `MLContext` object lost error in the GPU process is
// captured, it can send the error message to the renderer process and also
// handle the lost error in the renderer process.
interface WebNNContextClient {
  // Called to explain why the `WebNNContextClient` is no longer available.
  OnLost(string message);
};

// This interface runs in the GPU process and is called from the renderer
// process to create `WebNNContext` interface. The renderer requests this
// interface from the frame's BrowserInterfaceBroker, which requests it
// from the GpuService via the GpuClient.
[RuntimeFeature=webnn.mojom.features.kWebMachineLearningNeuralNetwork]
interface WebNNContextProvider {
  // Called by the renderer process to create a message pipe for `MLContext`,
  // the `CreateContextResult` will be `Error` with error messages if the
  // configuration of options isn't supported.
  CreateWebNNContext(CreateContextOptions options)
      => (CreateContextResult result);
};
